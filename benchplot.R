## Nice bar plot of grouping benchmark timings based on Matt Dowle scripts from 2014
## https://github.com/h2oai/db-benchmark/commit/fce1b8c9177afb49471fcf483a438f619f1a992b
## Original grouping benchmark can be found in: https://github.com/Rdatatable/data.table/wiki/Benchmarks-:-Grouping

groupby.code = list(
  "sum v1 by id1" = c(
    "dask"="x.groupby(['id1']).agg({'v1':'sum'}).compute()",
    "data.table"="DT[, .(v1=sum(v1)), keyby=id1]",
    "dplyr"="DF %>% group_by(id1) %>% summarise(sum(v1))",
    "juliadf"="by(x, :id1) do df; DataFrame(v1 = sum(df.v1)); end;",
    "pandas"="DF.groupby(['id1']).agg({'v1':'sum'})",
    "pydatatable"="DT[:, {'v1': sum(f.v1)}, f.id1]",
    "spark"="spark.sql('select sum(v1) as v1 from x group by id1')"
    ),
  "sum v1 by id1:id2" = c(
    "dask"="x.groupby(['id1','id2']).agg({'v1':'sum'}).compute()",
    "data.table"="DT[, .(v1=sum(v1)), keyby=.(id1, id2)]",
    "dplyr"="DF %>% group_by(id1,id2) %>% summarise(sum(v1))",
    "juliadf"="by(x, [:id1, :id2]) do df; DataFrame(v1 = sum(df.v1)); end;",
    "pandas"="DF.groupby(['id1','id2']).agg({'v1':'sum'})",
    "pydatatable"="DT[:, {'v1': sum(f.v1)}, [f.id1, f.id2]]",
    "spark"="spark.sql('select sum(v1) as v1 from x group by id1, id2')"
    ),
  "sum v1 mean v3 by id3" = c(
    "dask"="x.groupby(['id3']).agg({'v1':'sum', 'v3':'mean'}).compute()",
    "data.table"="DT[, .(v1=sum(v1), v3=mean(v3)), keyby=id3]",
    "dplyr"="DF %>% group_by(id3) %>% summarise(sum(v1), mean(v3))",
    "juliadf"="by(x, :id3) do df; DataFrame(v1 = sum(df.v1), v3 = mean(df.v3)); end;",
    "pandas"="DF.groupby(['id3']).agg({'v1':'sum', 'v3':'mean'})",
    "pydatatable"="DT[:, {'v1': sum(f.v1), 'v3': mean(f.v3)}, f.id3]",
    "spark"="spark.sql('select sum(v1) as v1, mean(v3) as v3 from x group by id3')"
    ),
  "mean v1:v3 by id4" = c(
    "dask"="x.groupby(['id4']).agg({'v1':'mean', 'v2':'mean', 'v3':'mean'}).compute()",
    "data.table"="DT[, lapply(.SD, mean), keyby=id4, .SDcols=v1:v3]",
    "dplyr"="DF %>% group_by(id4) %>% summarise_each(funs(mean), vars=7:9)",
    "juliadf"="by(x, :id4) do df; DataFrame(v1 = mean(df.v1), v2 = mean(df.v2), v3 = mean(df.v3)); end;",
    "pandas"="DF.groupby(['id4']).agg({'v1':'mean', 'v2':'mean', 'v3':'mean'})",
    "pydatatable"="DT[:, {'v1': mean(f.v1), 'v2': mean(f.v2), 'v3': mean(f.v3)}, f.id4]",
    "spark"="spark.sql('select mean(v1) as v1, mean(v2) as v2, mean(v3) as v3 from x group by id4')"
    ),
  "sum v1:v3 by id6" = c(
    "dask"="x.groupby(['id6']).agg({'v1':'sum', 'v2':'sum', 'v3':'sum'}).compute()",
    "data.table"="DT[, lapply(.SD, sum), keyby=id6, .SDcols=v1:v3]",
    "dplyr"="DF %>% group_by(id6) %>% summarise_each(funs(sum), vars=7:9)",
    "juliadf"="by(x, :id6) do df; DataFrame(v1 = sum(df.v1), v2 = sum(df.v2), v3 = sum(df.v3)); end;",
    "pandas"="DF.groupby(['id6']).agg({'v1':'sum', 'v2':'sum', 'v3':'sum'})",
    "pydatatable"="DT[:, {'v1': sum(f.v1), 'v2': sum(f.v2), 'v3': sum(f.v3)}, f.id6]",
    "spark"="spark.sql('select sum(v1) as v1, sum(v2) as v2, sum(v3) as v3 from x group by id6')"
    )
)

source("helpers.R") # for solution date based on git sha from github repositories, if no git revision available then hardcoded dictionary
stopifnot(sapply(c("curl","jsonlite"), requireNamespace, quietly=TRUE)) # used for lookup date based on git
library(data.table)
if (!capabilities()[["X11"]] && capabilities()[["cairo"]]) options(bitmapType="cairo") # fix for R compiled with-x=no with-cairo=yes

colors = rbindlist(list(
  list(solution="dplyr", colmain="red", collight="#FF7777"),
  list(solution="data.table", "blue", "#7777FF"),
  list(solution="pandas", "green4", "#77FF77"),
  list(solution="pydatatable", "darkorange", "orange"),
  list(solution="spark", "#8000FFFF", "#CC66FF"),
  list(solution="dask", "slategrey", "lightgrey"),
  list(solution="juliadf", "deepskyblue", "darkturquoise")
))

format_comma = function(x) format(as.integer(signif(x,4)), big.mark=",")

get_xlab_values = function(x) {
  at = pretty(x, 10)
  at[at!=0]
}

get_xlab_timescale = function(x) {
  if (x > 2*60*60) {
    timescale = 3600
    xlab = "Hours"
  } else if (x > 120) {
    timescale = 60
    xlab = "Minutes"
  } else {
    timescale = 1
    xlab = "Seconds"
  }
  setNames(timescale, xlab)
}

textBG = function(x, y, txt, w, ...) {
  txtw = strwidth(txt, ...); txth = strheight(txt, ...);
  txty = y-2*w;  # w from calling scope above
  rect(x, txty, x+txtw, txty+1.8*txth, col="white", border=NA)
  text(x, y, txt, adj=c(0, 0.7), ...)
}

# .nrow default Inf, numeric to filter out timingsto single in_rows, Inf will results to use maximum in_rows from timings
# task default "groupby", character scalar to filter out timings to single task, currently benchplot is used only for groupby task
# timings data.table of timings, structure as generated by `run.sh` script
# code list of syntax for solutions grouped by questions so syntax can be plotted next to the bars
# colors data.table of solutions and colors assigned to them
# interactive default interactive(), when TRUE it will print some output to console and open png after finished
# by.nsolutions default FALSE, when TRUE it will generate png filename as 'groupby.[nsolutions].[in_rows].png' so scaling of benchplot can be easily compared for various number of solutions
benchplot = function(.nrow=Inf, task="groupby", timings, code, colors, interactive=interactive(), by.nsolutions=FALSE) {
  
  stopifnot(c("task","time_sec","question","solution","in_rows","out_rows","out_cols","run","version","git","batch") %in% names(timings))
  stopifnot(is.character(task), length(task)==1L, !is.na(task))
  if (missing(code)) stop("provide 'code' argument, list of questions and respective queries in each solution")
  if (uniqueN(timings$batch)!=1L) stop("all timings to be presented has to be produced from same benchmark batch, `uniqueN(timings$batch)` must be equal to 1, there should be no NAs in 'batch' field")
  vbatch = timings$batch[1L]
  timings.task = unique(timings$task)
  if (length(intersect(timings.task, task))!=1L) stop("there should be only single task to present on benchplot, provide 'task' argument which exists in 'timings' dataset")
  vtask = task
  # filter timings to single task
  timings = timings[task==vtask]
  # if no .nrow argument provided then use maximum in_rows in timings
  if (!is.finite(.nrow)) .nrow = timings[, max(in_rows)]
  # filter timings to run 1 and 2
  timings = timings[run%in%c(1L, 2L)]
  
  exceptions = TRUE
  if (exceptions) {
    pandas_version = timings[solution=="pandas" & in_rows==min(in_rows), version[1L]]
    pandas_git = timings[solution=="pandas" & in_rows==min(in_rows), git[1L]]
    dask_version = timings[solution=="dask" & in_rows==min(in_rows), version[1L]]
    dask_git = timings[solution=="dask" & in_rows==min(in_rows), git[1L]]
  }
  
  # filter timings to single in_rows
  timings = timings[in_rows==.nrow]
  
  questions = unique(timings$question)
  nquestions = length(questions)
  runs = unique(timings$run)
  nruns = length(runs)
  data = unique(timings$data)
  ndata = length(data)
  if (ndata!=1L) stop("only single 'data' field supported, run benchplot for each 'data'")
  
  #timings[,.N,solution]
  if (exceptions) {
    # h2oai/datatable#1082 grouping by multiple cols not yet implemented, reset time_sec tot NA, impute out_rows and out_cols
    timings[solution=="pydatatable" & question=="sum v1 by id1:id2", time_sec:=NA_real_]
    if (timings[solution=="data.table", .N==0L]) stop("exception for pydatatable question2 is fixed based on data.table, you must have data.table solution included")
    fix_missing = timings[solution=="data.table" & question=="sum v1 by id1:id2", .(out_rows, out_cols)]
    timings[solution=="pydatatable" & question=="sum v1 by id1:id2", c("out_rows","out_cols") := fix_missing]
    
    # pandas 1e9 killed on 125GB machine due to not enough memory
    if (.nrow==1e9 && timings[solution=="pandas" & in_rows==1e9, uniqueN(question)*uniqueN(run)] < nquestions*nruns) {
      if (timings[solution=="data.table" & in_rows==1e9, .N==0L]) stop("exception for pandas 1e9 is fixed based on data.table, you must have data.table solution included")
      pandasi = timings[solution=="pandas" & in_rows==1e9, which=TRUE] # there might be some results, so we need to filter them out
      fix_pandas = timings[solution=="data.table" & in_rows==1e9
                           ][, time_sec:=NA_real_
                             ][, solution:="pandas"
                               ][, version:=pandas_version
                                 ][, git:=pandas_git]
      timings = rbindlist(list(timings[!pandasi], fix_pandas))[order(solution)]
    }
    # dask 1e9 killed on 125GB machine due to not enough memory
    if (.nrow==1e9 && timings[solution=="dask" & in_rows==1e9, uniqueN(question)*uniqueN(run)] < nquestions*nruns) {
      if (timings[solution=="data.table" & in_rows==1e9, .N==0L]) stop("exception for dask 1e9 is fixed based on data.table, you must have data.table solution included")
      daski = timings[solution=="dask" & in_rows==1e9, which=TRUE] # there might be some results, so we need to filter them out
      fix_dask = timings[solution=="data.table" & in_rows==1e9
                         ][, time_sec:=NA_real_
                           ][, solution:="dask"
                             ][, version:=dask_version
                               ][, git:=dask_git]
      timings = rbindlist(list(timings[!daski], fix_dask))[order(solution)]
    }
  }
  #timings[,.N,solution]
  
  solutions = unique(timings$solution)
  nsolutions = length(solutions)
  
  # get data size in GB from current directory by filename match
  gb = (if (file.exists(data)) file.info(data)$size else NA_real_)/1024^3
  
  # keep only required columns
  timings = timings[, .SD, .SDcols=c("time_sec","question","solution","in_rows","out_rows","out_cols","run","version","git","batch")]
  
  # cleanup git field, used when solution.date(only.date=FALSE)
  timings[git=="", git:=NA_character_]
  
  # add question order
  timings[as.data.table(list(question=questions))[, I:=.I], nquestion := i.I, on="question"]

  fnam = paste0(task, if (by.nsolutions) paste0(".", nsolutions) else "", ".", gsub("e[+]0", "E", pretty_sci(.nrow)), ".png")
  if (interactive) cat("Plotting to", fnam, "...\n")
  height = 700+120*nsolutions;
  png(file=fnam, width=800, height=height)

  mar.top = 3.1+nsolutions
  mar.bot = 3.3/nsolutions
  par(mar=c(mar.bot, 1.1, mar.top, 2.1)) # shift to the left: c(bottom, left, top, right)

  # veriyfy colors unique and defined for every solution
  stopifnot(colors[, .N==1L, .(solution, colmain, collight)]$V1)
  timings[colors, c("colmain","collight") := list(i.colmain, i.collight), on="solution"]
  stopifnot(timings[is.na(colmain) | is.na(collight), .N==0L])
  maincolors = rev(unique(timings$colmain))
  
  # define units of measure on X axis
  timescale = get_xlab_timescale(max(timings$time_sec, na.rm=TRUE))
  
  # prepare timings in expected unit of measure
  timings[, bars:=time_sec/timescale]
  
  # X axis values
  x_at = get_xlab_values(timings$bars) # we will need to do cut-off here
  
  # order for bar horiz=TRUE does first bar from the bottom!
  ans = timings[order(nquestion, solution, decreasing=TRUE)]
  
  # use padding to reserve extra space for solution syntax, and top X axis and its labels
  pad = as.vector(sapply(0:4, function(x) c(as.vector(rbind(x*nsolutions + 1:nsolutions, NA)), NA, NA)))
  # plot non colored bars of first timing to get `tt` positions
  tt = barplot(ans[run==1L, bars[pad]], horiz=TRUE, xlim=c(0, tail(x_at, 1L)), axes=FALSE)
  
  # plot timing values next to each bar
  max_t = ans[, pmax(bars[run==1L], bars[run==2L])] # max(run1, run2) otherwise bigger bar could overlaps text
  text(max_t, tt[!is.na(pad)]-0.15, round(max_t, 2), pos=4, cex=1.25) # round to 2 decimals places to avoid 0.0 cases
  
  # we reverse `tt` as horiz=TRUE does first bar from the bottom
  tt = rev(tt)
  # calculate half of bar width used later on in many places
  w = (tt[1]-tt[2])/4
  
  # upper line break to separate legend from timings
  h1 = tt[1]
  abline(h=h1)
  # X axis upper label (seconds, minutes) and values
  ff = if (length(x_at)<=8) TRUE else -1  # ff = first first X axis label overlap
  text(x=x_at[ff], y=h1, labels=format(x_at[ff]), adj=c(0.5, -0.5), cex=1.5, font=2, xpd=NA)
  text(x=0, y=h1, labels=names(timescale), adj=c(0, -0.5), font=2, cex=1.5, xpd=NA)
  
  # bottom horizontal line of X axis
  h2 = tail(tt, 1)-4*w
  abline(h=h2)
  # X axis lower label (seconds/minutes) and values
  text(x=x_at[ff], y=h2, labels=format(x_at[ff]), adj=c(0.5, 1.5), cex=1.5, font=2, xpd=NA)
  text(x=0, y=h2, labels=names(timescale), adj=c(0, 1.5), font=2, cex=1.5, xpd=NA)
  
  space = nsolutions*2 + 2
  # grey horizontal lines separating questions
  abline(h=tt[seq(space+1, by=space, length=4)], col="grey", lwd=2)
  # dotted vertical lines to form grid
  for (at_x in x_at) lines(x=c(at_x, at_x), y=c(h1, h2), col="lightgrey", lwd=2, lty="dotted")
  # color bars according to solutions
  barplot(ans[run==1L, bars[pad]], horiz=TRUE, axes=FALSE,
          col=rep(c(maincolors, "black"), each=2),
          font=2, xpd=NA, add=TRUE)
  
  # syntax to each question-solution, headers for each question
  for (iq in 1L:nquestions) {
    q = questions[iq]
    for (is in 1L:nsolutions) {
      s = solutions[is]
      cod = code[[q]][[s]]
      col = colors[s, colmain, on="solution"]
      textBG(0, tt[is*2L+1L+(iq-1)*space], txt=cod, w=w, col=col, font=2)
    }
    out_rows = ans[question==q & run==1L, out_rows]
    out_cols = ans[question==q & run==1L, out_cols]
    if (length(unique(out_rows)) != 1L) stop("out_rows mismatch")
    #if (length(unique(out_cols)) != 1L) stop("out_cols mismatch") # pd.ans.shape[1] does not return the actual columns and ans is pivot like
    out_rows = out_rows[1L]
    Mode = function(x) {tx<-table(x); as.numeric(names(tx)[which.max(tx)])}
    out_cols = Mode(out_cols) # pandas and spark does not return grouping column
    textBG(0, tt[2+(iq-1)*space], w=w, font=2,
           txt=sprintf("Question %s: %s ad hoc groups of %s rows;  result %s x %s",
                       iq, format_comma(out_rows), format_comma(.nrow/out_rows),
                       format_comma(out_rows), out_cols))
  }
  
  # bars of second timings (run==2L), or exceptions
  for (iq in 1L:nquestions) {
    q = questions[iq]
    for (is in 1L:nsolutions) {
      s = solutions[is]
      col = colors[s, collight, on="solution"]
      excol = colors[s, colmain, on="solution"]
      val = ans[solution==s & question==q & run==2L, bars]
      at = tt[(is+1)*2+(iq-1)*space]
      rect(0, at-w, val, at+w, col=col, xpd=NA)
      if (is.na(val)) { # we should use dictionary here instead of hardcoded
        exception = if (s%in%c("pandas","dask")) "Lack of memory to read data"
        else if (s=="pydatatable") "Not yet implemented"
        else "undefined exception"
        textBG(0, tt[(is+1)*2+(iq-1)*space], txt=exception, w=w, col=excol, font=2)
      }
    }
  }
  
  # cost per hour
  cph = 0.5 # minimum on graph histories; what people will see if they check

  # labels for legend solutions
  ans[, .(run12_time_sec = sum(time_sec, na.rm=TRUE), # total time of run 1 and run 2
          version = version[1], git = git[1]),
      keyby="solution"
      ][, sprintf(
        "%s %s  -  %s  -  Total: $%.02f for %s %s",
        if (solution=="pydatatable") "(py)datatable" else solution, # decode pydatatable to (py)datatable
        version,
        solution.date(solution, version, git, only.date=TRUE, use.cache=TRUE),
        cph*run12_time_sec/3600, # cost in dollars
        round(run12_time_sec/timescale, 0),
        tolower(names(timescale)) # minutes/seconds
      ),
        by="solution"
      ]$V1 -> leg
  # plot legend solutions
  topoffset = nsolutions*5-3
  legend_y = par()$usr[4]+topoffset*w # usr: c(x1, x2, y1, y2)
  legend(0, legend_y, pch=22, pt.bg=rev(maincolors), bty="n", cex=1.5, pt.cex=3.5,
         text.font=1, xpd=NA, legend=leg)
  mtext(sprintf("Input table: %s rows x %s columns ( %s GB )",
                format_comma(.nrow), 9L, # hardcoded number of columns!
                if (!is.na(gb)) { if (gb<1) round(gb, 1) else 5*round(ceiling(gb)/5) } else "NA"),
        line=1.5+nsolutions,
        side=3, cex=1.5, adj=0, font=2)
  # legend first/second timing
  legend(par()$usr[2], legend_y, pch=22, xpd=NA, xjust=1, bty="n", pt.lwd=1,
         legend=c("First time", "Second time"), pt.cex=c(3.5, 2.5), cex=1.5, pt.bg=colors[solution=="data.table", c(colmain, collight)])
  # footer timestamp of plot gen
  mtext(side=1, line=-1, text=format(as.POSIXct(vbatch, origin="1970-01-01"), usetz=TRUE), adj=1, outer=TRUE, cex=1)
  # put link to report
  mtext(side=1, line=-1, text=" https://h2oai.github.io/db-benchmark", adj=0, outer=TRUE, cex=1)
  dev.off()
  if (interactive) system(paste("/usr/bin/xdg-open",fnam), wait=FALSE) else invisible(TRUE)
}

if (dev<-FALSE) {
  d = fread("time.csv")[!is.na(batch)][batch==max(batch)]
  .nrow=1e9
  # verify scaling across number of solutions
  sols = list(c("data.table","pandas","dplyr"),
              c("data.table","pandas","dplyr","pydatatable"),
              c("data.table","spark","pydatatable","pandas","dplyr"),
              c("data.table","spark","pydatatable","pandas","dplyr","dask"),
              c("data.table","juliadf","dask","spark","pydatatable","pandas","dplyr"))
  for (s in sols) {
    benchplot(.nrow=.nrow, timings=d[solution%in%s], code=groupby.code, colors=colors, interactive=FALSE, by.nsolutions=TRUE)
  }
} else if (interactive()) {
  d = fread("time.csv")[!is.na(batch)][batch==max(batch)]
  .nrow=1e9
  timings=d; code=groupby.code; task="groupby"; interactive=TRUE; by.nsolutions=FALSE
  benchplot(.nrow=.nrow, timings=timings, code=code, colors=colors, interactive=interactive, by.nsolutions=by.nsolutions)
}

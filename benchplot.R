## Nice bar plot of grouping benchmark timings based on Matt Dowle scripts from 2014
## https://github.com/h2oai/db-benchmark/commit/fce1b8c9177afb49471fcf483a438f619f1a992b
## Original grouping benchmark can be found in: https://github.com/Rdatatable/data.table/wiki/Benchmarks-:-Grouping

groupby.code = list(
  "sum v1 by id1" = c(
    "dask"="x.groupby(['id1']).agg({'v1':'sum'}).compute()",
    "data.table"="DT[, .(v1=sum(v1)), by=id1]",
    "dplyr"="DF %>% group_by(id1) %>% summarise(sum(v1))",
    "juliadf"="by(x, :id1, v1 = :v1=>sum)",
    "pandas"="DF.groupby(['id1']).agg({'v1':'sum'})",
    "pydatatable"="DT[:, {'v1': sum(f.v1)}, by(f.id1)]",
    "spark"="spark.sql('select sum(v1) as v1 from x group by id1')"
    ),
  "sum v1 by id1:id2" = c(
    "dask"="x.groupby(['id1','id2']).agg({'v1':'sum'}).compute()",
    "data.table"="DT[, .(v1=sum(v1)), by=.(id1, id2)]",
    "dplyr"="DF %>% group_by(id1,id2) %>% summarise(sum(v1))",
    "juliadf"="by(x, [:id1, :id2], v1 = :v1=>sum)",
    "pandas"="DF.groupby(['id1','id2']).agg({'v1':'sum'})",
    "pydatatable"="DT[:, {'v1': sum(f.v1)}, by(f.id1, f.id2)]",
    "spark"="spark.sql('select sum(v1) as v1 from x group by id1, id2')"
    ),
  "sum v1 mean v3 by id3" = c(
    "dask"="x.groupby(['id3']).agg({'v1':'sum', 'v3':'mean'}).compute()",
    "data.table"="DT[, .(v1=sum(v1), v3=mean(v3)), by=id3]",
    "dplyr"="DF %>% group_by(id3) %>% summarise(sum(v1), mean(v3))",
    "juliadf"="by(x, :id3, v1 = :v1=>sum, v3 = :v3=>mean)",
    "pandas"="DF.groupby(['id3']).agg({'v1':'sum', 'v3':'mean'})",
    "pydatatable"="DT[:, {'v1': sum(f.v1), 'v3': mean(f.v3)}, by(f.id3)]",
    "spark"="spark.sql('select sum(v1) as v1, mean(v3) as v3 from x group by id3')"
    ),
  "mean v1:v3 by id4" = c(
    "dask"="x.groupby(['id4']).agg({'v1':'mean', 'v2':'mean', 'v3':'mean'}).compute()",
    "data.table"="DT[, lapply(.SD, mean), by=id4, .SDcols=v1:v3]",
    "dplyr"="DF %>% group_by(id4) %>% summarise_each(funs(mean), vars=7:9)",
    "juliadf"="by(x, :id4, v1 = :v1=>mean, v2 = :v2=>mean, v3 = :v3=>mean)",
    "pandas"="DF.groupby(['id4']).agg({'v1':'mean', 'v2':'mean', 'v3':'mean'})",
    "pydatatable"="DT[:, {'v1': mean(f.v1), 'v2': mean(f.v2), 'v3': mean(f.v3)}, by(f.id4)]",
    "spark"="spark.sql('select mean(v1) as v1, mean(v2) as v2, mean(v3) as v3 from x group by id4')"
    ),
  "sum v1:v3 by id6" = c(
    "dask"="x.groupby(['id6']).agg({'v1':'sum', 'v2':'sum', 'v3':'sum'}).compute()",
    "data.table"="DT[, lapply(.SD, sum), by=id6, .SDcols=v1:v3]",
    "dplyr"="DF %>% group_by(id6) %>% summarise_each(funs(sum), vars=7:9)",
    "juliadf"="by(x, :id6, v1 = :v1=>sum, v2 = :v2=>sum, v3 = :v3=>sum)",
    "pandas"="DF.groupby(['id6']).agg({'v1':'sum', 'v2':'sum', 'v3':'sum'})",
    "pydatatable"="DT[:, {'v1': sum(f.v1), 'v2': sum(f.v2), 'v3': sum(f.v3)}, by(f.id6)]",
    "spark"="spark.sql('select sum(v1) as v1, sum(v2) as v2, sum(v3) as v3 from x group by id6')"
    )
)

source("helpers.R") # for solution date based on git sha from github repositories, if no git revision available then hardcoded dictionary
stopifnot(sapply(c("curl","jsonlite"), requireNamespace, quietly=TRUE)) # used for lookup date based on git
library(data.table)
if (!capabilities()[["X11"]] && capabilities()[["cairo"]]) options(bitmapType="cairo") # fix for R compiled with-x=no with-cairo=yes

solution.colors = rbindlist(list(
  list(solution="dplyr", colmain="red", collight="#FF7777"),
  list(solution="data.table", "blue", "#7777FF"),
  list(solution="pandas", "green4", "#77FF77"),
  list(solution="pydatatable", "darkorange", "orange"),
  list(solution="spark", "#8000FFFF", "#CC66FF"),
  list(solution="dask", "slategrey", "lightgrey"),
  list(solution="juliadf", "deepskyblue", "darkturquoise")
))

format_comma = function(x) format(as.integer(signif(x,4)), big.mark=",")

get_xlab_values = function(x) {
  at = pretty(x, 10)
  at[at!=0]
}

get_xlab_timescale = function(x) {
  if (x > 2*60*60) {
    timescale = 3600
    xlab = "Hours"
  } else if (x > 120) {
    timescale = 60
    xlab = "Minutes"
  } else {
    timescale = 1
    xlab = "Seconds"
  }
  setNames(timescale, xlab)
}

textBG = function(x, y, txt, w, ...) {
  txtw = strwidth(txt, ...); txth = strheight(txt, ...);
  txty = y-2*w;  # w from calling scope above
  rect(x, txty, x+txtw, txty+1.8*txth, col="white", border=NA)
  text(x, y, txt, adj=c(0, 0.7), ...)
}

# .nrow default Inf, numeric to filter out timingsto single in_rows, Inf will results to use maximum in_rows from timings
# task default "groupby", character scalar to filter out timings to single task, currently benchplot is used only for groupby task
# timings data.table of timings, structure as generated by `run.sh` script
# code list of syntax for solutions grouped by questions so syntax can be plotted next to the bars
# colors data.table of solutions and colors assigned to them
# cutoff character name of solution to cutoff all longer timings
# cutoff.after numeric default 0.2, to cutoff after 120% percent of timing of solution provided to 'cutoff' argument, provide 0.5 for 150% and so on
# .interactive default interactive(), when TRUE it will print some output to console and open png after finished
# by.nsolutions default FALSE, when TRUE it will generate png filename as 'groupby.[nsolutions].[in_rows].png' so scaling of benchplot can be easily compared for various number of solutions
# fnam fixed filename if do not want to generate from pattern
benchplot = function(.nrow=Inf, task="groupby", data, timings, code, colors, cutoff="spark", cutoff.after=0.2, .interactive=interactive(), by.nsolutions=FALSE, fnam=NULL, path="public/plots") {
  stopifnot(c("task","time_sec","question","solution","in_rows","out_rows","out_cols","run","version","git","batch") %in% names(timings))
  stopifnot(is.character(task), length(task)==1L, !is.na(task))
  if (!is.data.table(colors)) stop("argument colors must be data.table of solutions and colors assigned to each")
  if (!is.character(cutoff) || length(cutoff)>1) stop("cutoff must be character of length 0 to 1")
  if (missing(code)) stop("provide 'code' argument, list of questions and respective queries in each solution")
  timings.task = unique(timings$task)
  if (length(intersect(timings.task, task))!=1L) stop("there should be only single task to present on benchplot, provide 'task' argument which exists in 'timings' dataset")
  vtask = task
  # filter timings to single task
  timings = timings[task==vtask]
  # if no .nrow argument provided then use maximum in_rows in timings
  if (!is.finite(.nrow)) .nrow = timings[, max(in_rows)]
  # filter timings to run 1 and 2
  timings = timings[run%in%c(1L, 2L)]
  
  exceptions = TRUE
  if (exceptions) {
    pandas_version = timings[solution=="pandas" & in_rows==min(in_rows), version[1L]]
    pandas_git = timings[solution=="pandas" & in_rows==min(in_rows), git[1L]]
    dask_version = timings[solution=="dask" & in_rows==min(in_rows), version[1L]]
    dask_git = timings[solution=="dask" & in_rows==min(in_rows), git[1L]]
    dplyr_version = timings[solution=="dplyr" & in_rows==min(in_rows), version[1L]]
    dplyr_git = timings[solution=="dplyr" & in_rows==min(in_rows), git[1L]]
  }
  
  # filter timings to single in_rows # this will be handled by filter on data
  #timings = timings[in_rows==.nrow]
  
  # filter timings to single data
  .data = data; rm(data)
  timings = timings[data==.data]
  if (!nrow(timings)) {
    message(sprintf("Nothing to plot for %s %s", task, .data))
    return(invisible(NULL))
  }
  if (uniqueN(timings$in_rows) != 1L) stop("There should be only single 'in_rows' after filtering on 'data'")
  
  questions = unique(timings$question)
  nquestions = length(questions)
  runs = unique(timings$run)
  nruns = length(runs)
  data = unique(timings$data)
  ndata = length(data)
  if (ndata!=1L) stop("only single 'data' field supported, run benchplot for each 'data'")
  
  #timings[,.N,solution]
  if (exceptions) {
    # pandas 1e9 killed on 125GB machine due to not enough memory
    if (.nrow==1e9 && timings[solution=="pandas" & in_rows==1e9, uniqueN(question)*uniqueN(run)] < nquestions*nruns) {
      if (timings[solution=="data.table" & in_rows==1e9, .N==0L]) stop("exception for pandas 1e9 is fixed based on data.table, you must have data.table solution included")
      pandasi = timings[solution=="pandas" & in_rows==1e9, which=TRUE] # there might be some results, so we need to filter them out
      fix_pandas = timings[solution=="data.table" & in_rows==1e9
                           ][, time_sec:=NA_real_
                             ][, solution:="pandas"
                               ][, version:=pandas_version
                                 ][, git:=pandas_git]
      timings = rbindlist(list(timings[!pandasi], fix_pandas))[order(solution)]
    }
    # dask 1e9 killed on 125GB machine due to not enough memory
    if (.nrow==1e9 && timings[solution=="dask" & in_rows==1e9, uniqueN(question)*uniqueN(run)] < nquestions*nruns) {
      if (timings[solution=="data.table" & in_rows==1e9, .N==0L]) stop("exception for dask 1e9 is fixed based on data.table, you must have data.table solution included")
      daski = timings[solution=="dask" & in_rows==1e9, which=TRUE] # there might be some results, so we need to filter them out
      fix_dask = timings[solution=="data.table" & in_rows==1e9
                         ][, time_sec:=NA_real_
                           ][, solution:="dask"
                             ][, version:=dask_version
                               ][, git:=dask_git]
      timings = rbindlist(list(timings[!daski], fix_dask))[order(solution)]
    }
    # dplyr fails on 1e9 k=2
    if (data=="G1_1e9_2e0_0_0" && .nrow==1e9 && timings[solution=="dplyr" & in_rows==1e9 & data=="G1_1e9_2e0_0_0", uniqueN(question)*uniqueN(run)] < nquestions*nruns) {
      if (timings[solution=="data.table" & in_rows==1e9 & data=="G1_1e9_2e0_0_0", .N==0L]) stop("exception for dplyr 1e9 k=2e0 is fixed based on data.table, you must have data.table solution included")
      dplyri = timings[solution=="dplyr" & in_rows==1e9 & data=="G1_1e9_2e0_0_0", which=TRUE] # there might be some results, so we need to handle them nicely
      fix_dplyr = timings[solution=="data.table" & in_rows==1e9 & data=="G1_1e9_2e0_0_0"][!timings[dplyri, .(question, run)], on=c("question","run")
                           ][, time_sec:=NA_real_
                             ][, solution:="dplyr"
                               ][, version:=dplyr_version
                                 ][, git:=dplyr_git
                                   ][, `:=`(mem_gb=NA_real_, fun=NA_character_, chk_time_sec=NA_real_)]
      timings = rbindlist(list(timings, fix_dplyr))[order(solution)]
    }
  }
  #timings[,.N,solution]
  
  solutions = unique(timings$solution)
  nsolutions = length(solutions)
  
  if (length(cutoff) && !cutoff%in%solutions) stop(sprintf("'cutoff' argument used but provided value '%s' is not a solution existing in timing data", cutoff))
  
  # get data size in GB from current directory by filename match
  gb = (if (file.exists(data)) file.info(data)$size else NA_real_)/1024^3
  
  # keep only required columns
  timings = timings[, .SD, .SDcols=c("time_sec","question","solution","in_rows","out_rows","out_cols","run","version","git","batch")]
  
  # cleanup git field, used when solution.date(only.date=FALSE)
  timings[git=="", git:=NA_character_]
  
  # add question order
  timings[as.data.table(list(question=questions))[, I:=.I], nquestion := i.I, on="question"]
  
  if (is.null(fnam)) fnam = paste0(task, if (by.nsolutions) paste0(".", nsolutions) else "", ".", gsub("e[+]0", "E", pretty_sci(.nrow)), ".png")
  if (!is.null(path)) {
    if (!dir.exists(path)) dir.create(path, recursive=TRUE)
    fnam = file.path(path, fnam)
  }
  if (.interactive) cat("Plotting to", fnam, "...\n")
  height = 700+120*nsolutions;
  png(file=fnam, width=800, height=height)

  mar.top = 3.1+nsolutions
  mar.bot = 3.3/nsolutions
  par(mar=c(mar.bot, 1.1, mar.top, 2.1)) # shift to the left: c(bottom, left, top, right)
  
  # veriyfy colors unique and defined for every solution
  stopifnot(colors[, .N==1L, .(solution, colmain, collight)]$V1)
  timings[colors, c("colmain","collight") := list(i.colmain, i.collight), on="solution"]
  stopifnot(timings[is.na(colmain) | is.na(collight), .N==0L])
  
  # cutoff
  .cutoff = cutoff; rm(cutoff)
  timings[, cutoff:=FALSE]
  if (length(.cutoff)) {
    cutoff.time = timings[solution==.cutoff, max(time_sec, na.rm=TRUE)]
    cutoff.time.after = cutoff.time * (1+cutoff.after)
    if (is.na(cutoff.time)) stop("cutoff.time value is NA")
    timings[time_sec > cutoff.time.after, "cutoff":=TRUE]
  }
  
  # define units of measure on X axis
  timescale = get_xlab_timescale(timings[cutoff==FALSE, max(time_sec, na.rm=TRUE)])
  
  # prepare timings in expected unit of measure
  timings[, bars:=time_sec/timescale][, cutoff_bars:=bars]
  timings[cutoff==TRUE, bars:=NA_real_]
  cutoff.bars.after = if (length(.cutoff)) cutoff.time.after/timescale else 0
  
  # X axis values
  x_at = timings[, get_xlab_values(c(bars, cutoff.bars.after))]
  
  # order for bar horiz=TRUE does first bar from the bottom!
  ans = timings[, .SD][, max_time_sec_runs:=max(time_sec), by=.(solution, question)][order(nquestion, max_time_sec_runs, na.last=FALSE, decreasing=TRUE)]
  
  # use padding to reserve extra space for solution syntax, and top X axis and its labels
  pad = as.vector(sapply(0:4, function(x) c(as.vector(rbind(x*nsolutions + 1:nsolutions, NA)), NA, NA)))
  # plot non colored bars of first timing to get `tt` positions
  tt = barplot(ans[run==1L, bars[pad]], horiz=TRUE, xlim=c(0, tail(x_at, 1L)), axes=FALSE)
  
  # we reverse `tt` as horiz=TRUE does first bar from the bottom
  tt = rev(tt)
  # calculate half of bar width used later on in many places
  w = (tt[1]-tt[2])/4
  
  # upper line break to separate legend from timings
  h1 = tt[1]
  abline(h=h1)
  # X axis upper label (seconds, minutes) and values
  ff = if (length(x_at)<=8) TRUE else -1  # ff = first first X axis label overlap
  text(x=x_at[ff], y=h1, labels=format(x_at[ff]), adj=c(0.5, -0.5), cex=1.5, font=2, xpd=NA)
  text(x=0, y=h1, labels=names(timescale), adj=c(0, -0.5), font=2, cex=1.5, xpd=NA)
  
  # bottom horizontal line of X axis
  h2 = tail(tt, 1)-4*w
  abline(h=h2)
  # X axis lower label (seconds/minutes) and values
  text(x=x_at[ff], y=h2, labels=format(x_at[ff]), adj=c(0.5, 1.5), cex=1.5, font=2, xpd=NA)
  text(x=0, y=h2, labels=names(timescale), adj=c(0, 1.5), font=2, cex=1.5, xpd=NA)
  
  space = nsolutions*2 + 2
  # grey horizontal lines separating questions
  abline(h=tt[seq(space+1, by=space, length=4)], col="grey", lwd=2)
  # dotted vertical lines to form grid
  for (at_x in x_at) lines(x=c(at_x, at_x), y=c(h1, h2), col="lightgrey", lwd=2, lty="dotted")
  
  # first run bars according to solutions
  ans[run==1L, #.(cutoff_bars[pad], c(colmain,"black")[pad])
      barplot(cutoff_bars[pad], horiz=TRUE, axes=FALSE,
              col=c(colmain,"black")[pad],
              font=2, xpd=NA, add=TRUE)
      ]
  
  # syntax to each question-solution, headers for each question
  for (iq in 1L:nquestions) {
    q = questions[iq]
    q_ord_solutions = union(
      ans[nquestion==iq & run==1L][order(max_time_sec_runs, na.last=TRUE), solution],
      solutions # undefined exceptions
    )
    # plot syntax
    for (is in 1L:nsolutions) {
      s = q_ord_solutions[is]
      cod = code[[q]][[s]]
      col = colors[s, colmain, on="solution"]
      textBG(0, tt[is*2L+1L+(iq-1)*space], txt=cod, w=w, col=col, font=2)
    }
    # plot headers
    out_rows = ans[question==q & run==1L, out_rows]
    out_cols = ans[question==q & run==1L, out_cols]
    if (length(unique(out_rows)) != 1L) stop("out_rows mismatch")
    #if (length(unique(out_cols)) != 1L) stop("out_cols mismatch") # pd.ans.shape[1] does not return the actual columns and ans is pivot like
    out_rows = out_rows[1L]
    Mode = function(x) {tx<-table(x); as.numeric(names(tx)[which.max(tx)])}
    out_cols = Mode(out_cols) # pandas and spark does not return grouping column
    textBG(0, tt[2+(iq-1)*space], w=w, font=2,
           txt=sprintf("Question %s: %s ad hoc groups of %s rows;  result %s x %s",
                       iq, format_comma(out_rows), format_comma(.nrow/out_rows),
                       format_comma(out_rows), out_cols))
  }
  
  # bars of second timings (run==2L), or exceptions
  for (iq in 1L:nquestions) {
    q = questions[iq]
    q_ord_solutions = union(
      ans[nquestion==iq & run==1L][order(max_time_sec_runs, na.last=TRUE), solution],
      solutions
    )
    for (is in 1L:nsolutions) {
      s = q_ord_solutions[is]
      col = colors[s, collight, on="solution"]
      excol = colors[s, colmain, on="solution"]
      val = ans[solution==s & question==q & run==2L, cutoff_bars]
      at = tt[(is+1)*2+(iq-1)*space]
      rect(0, at-w, val, at+w, col=col, xpd=NA)
      if (is.na(val)) { # we should use dictionary here instead of hardcoded
        exception = if (s%in%c("pandas","dask")) "Lack of memory to read data"
        else if (s%in%c("dplyr")) "Cannot allocate memory"
        else "undefined exception"
        textBG(0, tt[(is+1)*2+(iq-1)*space], txt=exception, w=w, col=excol, font=2)
      }
    }
  }
  
  # plot timing values next to each bar
  max_t = ans[, pmax(cutoff_bars[run==1L], cutoff_bars[run==2L])] # max(run1, run2) otherwise bigger bar could overlaps text
  max_t_x_pos = max_t
  if (length(.cutoff)) max_t_x_pos[max_t>cutoff.bars.after] = x_at[length(x_at)-1L] #cutoff.bars.after
  max_t_y_pos = rev(tt)[!is.na(pad)]-w/2
  text(max_t_x_pos, max_t_y_pos, round(max_t, 2), pos=4, cex=1.25) # round to 2 decimals places to avoid 0.0 cases
  
  # cost per hour
  cph = 0.5 # minimum on graph histories; what people will see if they check
  
  # legend location
  topoffset = nsolutions*5-3
  legend_y = par()$usr[4]+topoffset*w # usr: c(x1, x2, y1, y2)
  
  # legend header
  mtext(sprintf("Input table: %s rows x %s columns ( %s GB )",
                format_comma(.nrow), 9L, # hardcoded number of columns!
                if (!is.na(gb)) { if (gb<1) round(gb, 1) else 5*round(ceiling(gb)/5) } else "NA"),
        line=1.5+nsolutions,
        side=3, cex=1.5, adj=0, font=2)
  # legend first/second timing box
  legend(par()$usr[2], legend_y, pch=22, xpd=NA, xjust=1, bty="n", pt.lwd=1,
         legend=c("First time", "Second time"), pt.cex=c(3.5, 2.5), cex=1.5, pt.bg=colors[solution=="data.table", c(colmain, collight)])
  
  # legend
  ans[, .(run12_time_sec = sum(time_sec), # total time of run 1 and run 2
          batch=batch[1], version = version[1], git = git[1], colmain=colmain[1]),
      keyby="solution"
      ][order(run12_time_sec, na.last=TRUE)
        ][, .(leg=sprintf(
        "%s %s  -  %s  -  Total: $%.02f for %s %s",
        if (solution=="pydatatable") "(py)datatable" else if (solution=="juliadf") "DataFrames.jl" else solution, # decode names
        version,
        format(as.Date(as.POSIXct(as.numeric(batch), origin="1970-01-01"))), # solution.date(solution, version, git, only.date=TRUE, use.cache=TRUE),
        cph*run12_time_sec/3600, # cost in dollars
        round(run12_time_sec/timescale, 0),
        tolower(names(timescale)) # minutes/seconds
      ), colmain=colmain),
      by="solution"
      ][, legend(0, legend_y, pch=22, pt.bg=colmain, bty="n", cex=1.5, pt.cex=3.5,
                 text.font=1, xpd=NA, legend=leg)] -> nul
  
  # footer timestamp of plot gen
  mtext(side=1, line=-1, text=format(Sys.time(), usetz=TRUE), adj=1, outer=TRUE, cex=1)
  # put link to report
  mtext(side=1, line=-1, text=" https://h2oai.github.io/db-benchmark", adj=0, outer=TRUE, cex=1)
  dev.off()
  if (.interactive) system(paste("/usr/bin/xdg-open",fnam), wait=FALSE) else invisible(TRUE)
}

if (dev1<-FALSE) {
  stop("adjust non-max batch only")
  d = fread("time.csv")[!is.na(batch)][batch==max(batch)]
  .nrow=1e9
  # verify scaling across number of solutions
  sols = list(c("data.table","pandas","dplyr"),
              c("data.table","pandas","dplyr","pydatatable"),
              c("data.table","spark","pydatatable","pandas","dplyr"),
              c("data.table","spark","pydatatable","pandas","dplyr","dask"),
              c("data.table","juliadf","dask","spark","pydatatable","pandas","dplyr"))
  for (s in sols) {
    benchplot(.nrow=.nrow, timings=d[solution%in%s], code=groupby.code, colors=solution.colors, .interactive=FALSE, by.nsolutions=TRUE)
  }
} else if (dev2<-FALSE) {
  d = fread("time.csv")[!is.na(batch)][in_rows %in% c(1e7, 1e8, 1e9)]
  recent = d[, .(max_batch=max(batch)), .(solution, task, data)]
  d = d[recent, on=c("solution","task","data","batch"="max_batch"), nomatch=NULL]
  .nrow=1e9
  timings=d; code=groupby.code; task="groupby"; .interactive=TRUE; by.nsolutions=FALSE; cutoff="spark"; cutoff.after=0.2; data="G1_1e9_1e2_0_0"; fnam=NULL; path=NULL
  benchplot(.nrow=.nrow, data=data, timings=timings, code=code, colors=solution.colors, cutoff=cutoff, .interactive=.interactive, by.nsolutions=by.nsolutions)
}

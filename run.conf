# this file will be sourced at start of run.sh
#
# glossary:
# client - 1 machine acts like a client, also driver in spark, the session where you call run.sh
# master - 1 machine which coordinate slaves work, if any, h2o doesn't have master
# slaves - N machines that execute the job
# cluster - master and slaves
#
# requirements:
# run one-time setup/setup-*.sh to bootstrap apps to cluster
# SRC_X and SRC_Y pointing to valid "hdfs://" paths for join datasets
# SRC_GRP pointing file for groupby
# passwordless ssh from client to cluster
# ~/tmp dir must exists on every machine
# impala cluster must be aready running, and $IMPALA_MASTER as host to any impala daemon
# spark and h2o clusters will be started during the script

# hostname
export MASTER="mr-0xd8"
export SLAVES="mr-0xd1 mr-0xd2 mr-0xd3 mr-0xd4 mr-0xd5 mr-0xd7 mr-0xd9 mr-0xd10"
export CLUSTER="$MASTER $SLAVES"

# task
export RUN_TASKS="join groupby"

# source data
# use data.csv

# output
export CSV_TIME_FILE="time.csv"

# spark config
export SPARK_PRINT_LAUNCH_COMMAND=1
export SPARK_MASTER_IP=$MASTER
export SPARK_MASTER_PORT="17077"
export SPARK_WORKER_IP=$SLAVES
export SPARK_MASTER="spark://$SPARK_MASTER_IP:$SPARK_MASTER_PORT"
export SPARK_HOME="$HOME/spark-2.0.0-SNAPSHOT-bin-hadoop2.6"
export SPARK_LOG_DIR=~/tmp/spark/logs # not /tmp as that's limited to 50GB and got full with 1e9 test: 'No space left on device'
export SPARK_WORKER_DIR=~/tmp/spark/work
export SPARK_LOCAL_DIRS=~/tmp/spark/work

# impala
export IMPALA_HOME="$HOME/impala"
export IMPALA_MASTER="mr-0xd2-precise1" # can be any as there is no master node in impala

# h2o
export MEM="-Xmx200G -Xms200G"
export H2O_HOST=$MASTER # can be any as there is no master node in h2o
export H2O_PORT="55888"
export H2O_NAME="${USER}_H2O"

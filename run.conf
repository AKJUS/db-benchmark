# this file will be sourced at start of run.sh
#
# glossary:
# client - 1 machine acts like a client, also driver in spark, the session where you call run.sh
# master - 1 machine which coordinate slaves work, if any, h2o doesn't have master
# slaves - N machines that execute the job
# cluster - master + slaves
#
# requirements:
# run one-time setup/setup-*.sh to bootstrap apps to cluster
# SRC_X and SRC_Y pointing to valid "hdfs://" path
# passwordless ssh from client to cluster
# ~/tmp dir must exists on every machine
# impala cluster must be aready running, and IMPALA_MASTER as host to any daemon
# spark and h2o clusters will be started during script

# hostname
export MASTER="mr-0xd8"
export SLAVES="mr-0xd1 mr-0xd2 mr-0xd3 mr-0xd4 mr-0xd5 mr-0xd7 mr-0xd9 mr-0xd10"
export CLUSTER="$MASTER $SLAVES"

# source data
export SRC_X="hdfs://mr-0xd6/datasets/mattd/X1e7_2c.csv" # use 1e7/1e8/1e9 and re-run
export SRC_Y="hdfs://mr-0xd6/datasets/mattd/Y1e7_2c.csv"
export SRC_X_LOCAL=$(echo "$SRC_X" | cut -c 15-) # extract path for impala: /datasets/mattd/X1e7_2c.csv - for impala and pandas
export SRC_Y_LOCAL=$(echo "$SRC_Y" | cut -c 15-)

# output
export CSV_TIME_FILE="time.csv"

# spark config
export SPARK_PRINT_LAUNCH_COMMAND=1
export SPARK_MASTER_IP=$MASTER
export SPARK_MASTER_PORT="17077"
export SPARK_WORKER_IP=$SLAVES
export SPARK_MASTER="spark://$SPARK_MASTER_IP:$SPARK_MASTER_PORT"
export SPARK_HOME="$HOME/spark-2.0.0-SNAPSHOT-bin-hadoop2.6"
export SPARK_LOG_DIR=~/tmp/spark/logs # not /tmp as that's limited to 50GB and got full with 1e9 test: 'No space left on device'
export SPARK_WORKER_DIR=~/tmp/spark/work
export SPARK_LOCAL_DIRS=~/tmp/spark/work

# impala
export IMPALA_HOME="$HOME/impala"
export IMPALA_MASTER="mr-0xd2-precise1" # can be any as there is no master node in impala

# h2o
export MEM="-Xmx200G -Xms200G"
export H2O_HOST=$MASTER # can be any as there is no master node in h2o
export H2O_PORT="55888"
export H2O_NAME="${USER}_H2O"

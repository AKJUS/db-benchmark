---
title: "db-benchmark"
output:
  html_document:
    dev: svg
---

```{r load_deps, include=FALSE}
knitr::opts_chunk$set(cache=FALSE)
# Rscript -e 'rmarkdown::render("index.Rmd")'
stopifnot(sapply(c("DT","jsonlite","lattice"), requireNamespace))
library(bit64)
library(data.table)
source("helpers.R")
source("benchplot.R")
```

```{r exceptions, include=FALSE}
L64 = function(...) as.integer64(unlist(list(...)))
exceptions = rbindlist(list(
  data.table(solution = "pandas", version="0.23.0", task = "groupby", in_rows = L64(1e9), data=c("G1_1e9_1e2.csv"), comment = "lack of memory"),
  data.table(solution = "pydatatable", version="0.6.0", task = "join", in_rows = L64(1e7,1e8,1e9), data=c("X1e7_2c.csv-Y1e7_2c.csv","X1e8_2c.csv-Y1e8_2c.csv","X1e9_2c.csv-Y1e9_2c.csv"), comment = "not yet implemented")
))
```

```{r load_data, include=FALSE}
# all timings
DT = read_timing("time.csv", col=NULL)
# recent timings, single cache=FALSE scenario where available
dt = last_timing(x=DT)
dt = dt[in_rows %in% L64(1e7, 1e8, 1e9, 2e9, 5e9, 1e10)] # filter out 1e6, and eventually others
metadata = fread("data.csv") # dictionary of data size
```

```{r def_funs, include=FALSE}
plot_op = function(subset_in_rows, subset_task, dt, subset_solution) {
  stopifnot(is.data.table(dt), is.numeric(subset_in_rows), length(subset_in_rows)==1L, is.character(subset_task), length(subset_task)==1L)
  toplot = dt[in_rows==subset_in_rows & task==subset_task]
  if (!missing(subset_solution)) toplot = toplot[solution%in%subset_solution]
  if (!nrow(toplot)) return(invisible())
  do_exceptions = exists("exceptions") && is.data.table(exceptions) && nrow(exceptions)
  if (do_exceptions) {
    msg = exceptions[in_rows==subset_in_rows & task==subset_task]
    if (!missing(subset_solution)) msg = msg[solution%in%subset_solution]
    common = toplot[msg, nomatch=0L, on=c("task","data","in_rows","solution")]
    if (nrow(common)) {
      # this message will be printed to console output while rendering, not to the document, after upgrade version exceptions should be re-confirmed and updated/removed if needed
      common[, message(sprintf("Timings for '%s' has been filtered out due to exception defined: %s", paste(unique(solution), collapse=", "), paste(unique(i.comment), collapse=", ")))]
      toplot = toplot[!msg, on=c("task","data","in_rows","solution")]
    }
    toplot = rbindlist(list(toplot, msg), use.names=TRUE, fill=TRUE)
  }
  # report averaging, see description below, it is reported in plot main title, useful to track unexpected averaging, or when `DT` (all timings) passed instead of `dt` (last timings)
  n_obs = sort(unique(toplot[!is.na(timestamp), .N, .(task, in_rows, solution, run)]$N)) # is.na(timestamp) filter outs exceptions
  toplot[,
         .(time_sec=mean(time_sec)), # average over question and datasets (not yet used) and cache (only if all timings provided: `DT`, not `dt`)
         .(task, in_rows, solution, version, run, exception=comment)
         ][, time_min:=time_sec/60
           ] -> toplot
  stopifnot(toplot[is.na(exception), uniqueN(run), .(solution)]$V1==3L) # incomplete runs makes the plot invalid
  toplot[!is.na(exception) & is.na(run), run := 1L] # fill exceptions with run=1
  # use secs/min dynamically
  if (max(toplot$time_sec, na.rm=TRUE) <= 3*60) {
    toplot[, time := time_sec]
    time_unit = "Seconds"
  } else {
    toplot[, time := time_min]
    time_unit = "Minutes"
  }
  toplot[, `:=`(solution = paste(solution, substr(version, 1, 8)))][, version:=NULL]
  toplot[run==1, ord := rank(time)][, ord := ord[1L], .(task, in_rows, solution)] # recycle order for exceptions
  par(mar=c(2.5,8,4,2)+0.1)
  xrn = range(c(0,toplot$time), na.rm=TRUE)
  nsol = uniqueN(toplot$solution)
  toplot[run==1, plot(time,ord,pch=19,cex=3,ylim=c(0,.N),xlim=xrn,col="red",axes=FALSE,ann=FALSE)]
  toplot[run==1, mtext(exception, side=4, at=seq_along(solution), font=1.25, adj=1, las=1, cex=1.25)]
  toplot[run==2, points(time,ord,pch=19,cex=2.5,col="blue")]
  toplot[run==3, points(time,ord,pch=19,cex=2,col="green")]
  axis(1, cex.axis=1.5, font=2, padj=0.5, line=-1)
  toplot[run==1][order(ord), mtext(solution, side=2, at=seq_along(solution), font=1.25, las=1, cex=1.25)]
  mtext(paste0(time_unit, "  "), side=2, 
        at=if (nsol==8) -0.65 else if (nsol==5) -0.4625 else if (nsol==4) -0.325 else -0.5, # dynamic based on number of solutions, tested on 8 and 5
        las=1, cex=1.5, font=2)
  gb = metadata[task==subset_task & rows==subset_in_rows, sum(gb)]
  if (length(gb)==0 || is.na(gb)) {
    gb = "Unknown GB"
  } else if (length(gb)==1L) {
    if (gb > 1) gb = round(gb, 0)
    gb = paste0(gb, "GB")
  } else {
    stop("length of 'gb' from metadata > 1, which one to use? investigate")
  }
  title(main = sprintf("%s %s%s", gb, subset_task,
                       if (identical(n_obs, 1L)) "" else paste0(" (mean over ",paste(n_obs, collapse=",")," queries)")))
  invisible()
}

plot_all = function(dt, what, subset_task) {
  stopifnot(is.data.table(dt), is.character(subset_task), length(subset_task)==1L, is.character(what), what%in%c("cpu","mem"), length(what)==1L)
  if (dt[task==subset_task, .N]==0L) return(invisible())
  pad = lattice::lattice.getOption("axis.padding")
  pad$numeric = 0 #0.07
  lattice::lattice.options("axis.padding" = pad)
  png(sprintf("%s-%s.png", subset_task, what), width=1080, height=720)
  p = if (what=="mem") lattice::barchart(
  mem_gb ~ run |
    factor(question, levels=unique(question)) +
    factor(in_rows, levels=lvls<-rev(as.character(unique(in_rows))),
           labels=sapply(seq_along(lvls), function(i) pretty_sci(lvls[i]))),
  data = dt, group = paste(solution, version), ylim=c(0, NA),
  horizontal=FALSE, subset = task==subset_task, auto.key=TRUE, scales=list(relation="free"),
  main = paste(subset_task, "RSS memory usage - as of", paste(format(as.POSIXct(dt[, unique(range(batch))], origin="1970-01-01"), "%Y-%m-%d"), collapse="~")), xlab = "run", ylab = "GB"
) else if (what=="cpu") lattice::barchart(
    time_sec ~ run |
      factor(question, levels=unique(question)) +
      factor(in_rows, levels=lvls<-rev(as.character(unique(in_rows))),
           labels=sapply(seq_along(lvls), function(i) pretty_sci(lvls[i]))),
    data = dt, groups = paste(solution, version), ylim=c(0, NA),
    horizontal=FALSE, subset = task==subset_task, auto.key=TRUE, scales=list(relation="free"),
    main = paste(subset_task, "time elapsed - as of", paste(format(as.POSIXct(dt[, unique(range(batch))], origin="1970-01-01"), "%Y-%m-%d"), collapse="~")), xlab = "run", ylab = "seconds"
  )
  print(p)
  dev.off()
}

task_source = function(subset_task, DT) {
  stopifnot(length(subset_task)==1L)
  cat(sapply(DT[task==subset_task, unique(solution)], function(x) {
    # remove dots from solution name
    fp = file.path("https://github.com/h2oai/db-benchmark/blob/master", xx<-gsub(".", "", x, fixed=TRUE), sprintf("%s-%s.%s", subset_task, xx, file.ext(x)))
    sprintf("- [%s](%s)", basename(fp), fp)
  }), sep="  \n")
}
```

## Benchmark overview

Repository for reproducible benchmarking of database-like operations.  
Benchmark is mainly focused on portability and reproducibility. I encourage readers to re-run benchmark on own machine, see [h2oai/db-benchmark](https://github.com/h2oai/db-benchmark) readme for details. Pull requests are welcome, any changes proposed should be portable.  

Time includes the actual query time and a call to verify dimensions of an answer, `dim` in R or `.shape` in python, to ensure no lazy evaluation is taking place.  
Size of data mentioned in plots refer to on-disk stored csv file, or in case of multiple tables involved, the sum of csv files size.  

Following legend describe rank plots in next sections.  

```{r o_legend, echo=FALSE, fig.width=1.85, fig.height=1.2}
oldmar = par(mar=rep(0,4))
plot(0,0,type="n", axes=F, xlab="", ylab="")
legend("center", legend=c("1st","2nd","3rd"), title="Each query runs 3 times", col=c("red","blue","green"), pch=19, pt.cex=c(3,2.5,2), bty="n", y.intersp = 1.25, inset = 0.1)
par(oldmar)
```

------

## High cardinality big to big join {.tabset .tabset-fade .tabset-pills}

```{r o_join0, echo=FALSE, include=FALSE}
plot_all(dt, "cpu", "join")
plot_all(dt, "mem", "join")
```

Detailed summary of benchmark available for [cpu](join-cpu.png) and [memory](join-mem.png)  

### 1e7

```{r o_join1, echo=FALSE}
plot_op(1e7, "join", dt)
```

### 1e8

```{r o_join2, echo=FALSE}
plot_op(1e8, "join", dt)
```

### 1e9 {.active}

```{r o_join3, echo=FALSE}
plot_op(1e9, "join", dt)
```

### 5e9

```{r o_join4, echo=FALSE, message=FALSE}
plot_op(5e9, "join", dt)
```

### 1e10

```{r o_join5, echo=FALSE, message=FALSE}
plot_op(1e10, "join", dt)
```


## Sort {.tabset .tabset-fade .tabset-pills}

```{r o_sort0, echo=FALSE, include=FALSE}
plot_all(dt, "cpu", "sort")
plot_all(dt, "mem", "sort")
```

Detailed summary of benchmark available for [cpu](sort-cpu.png) and [memory](sort-mem.png)  

### 1e7

```{r o_sort1, echo=FALSE}
plot_op(1e7, "sort", dt)
```

### 1e8

```{r o_sort2, echo=FALSE}
plot_op(1e8, "sort", dt)
```

### 1e9 {.active}

```{r o_sort3, echo=FALSE}
plot_op(1e9, "sort", dt)
```

### 5e9

```{r o_sort4, echo=FALSE, message=FALSE}
plot_op(5e9, "sort", dt)
```

### 1e10

```{r o_sort5, echo=FALSE, message=FALSE}
plot_op(1e10, "sort", dt)
```

## Group by {.tabset .tabset-fade .tabset-pills}

```{r o_groupby0, echo=FALSE, include=FALSE}
plot_all(dt, "cpu", "groupby")
plot_all(dt, "mem", "groupby")
```

Detailed summary of benchmark available for [cpu](groupby-cpu.png) and [memory](groupby-mem.png)  

### 1e7

[Detailed 1e7 benchplot](grouping.1e7.png)  

```{r o_groupby1, echo=FALSE}
plot_op(1e7, "groupby", dt)
benchplot(1e7)
```

### 1e8

[Detailed 1e8 benchplot](grouping.1e8.png)  

```{r o_groupby2, echo=FALSE}
plot_op(1e8, "groupby", dt)
benchplot(1e8)
```

### 1e9 {.active}

[Detailed 1e9 benchplot (now measures are messed up, will work fine once pandas pass 1e9)](grouping.1e9.png)  

```{r o_groupby3, echo=FALSE}
plot_op(1e9, "groupby", dt)
benchplot(1e9)
```

### 5e9

```{r o_groupby4, echo=FALSE, message=FALSE}
plot_op(5e9, "groupby", dt)
```

### 1e10

```{r o_groupby5, echo=FALSE, message=FALSE}
plot_op(1e10, "groupby", dt)
```

------

## Benchmark description

## Join description {.tabset .tabset-fade .tabset-pills}

### Notes {.active}

- join results will be included after [h2oai/datatable#1080](https://github.com/h2oai/datatable/issues/1080)

### Test scripts

```{r join_source, echo=FALSE, results='asis'}
task_source("join", DT)
```

### Data

data size: 

```{r join_data_size, echo=FALSE,}
metadata[task=="join", .N, .(rows, gb)][, .(`Rows`=rows, GB=gb)]
```

example input:

```{r join_data_head, echo=FALSE}
cat("X:")
data.table(KEY = c(829673L, 5501052L, 9635168L, 7069052L), X2 = c(-92335L, -8190789L, -6631465L, -1289657L))
cat("Y:")
data.table(KEY = c(6101982L, 8723957L, 3409724L, 230673L), Y2 = c(3226855L, -8875053L, 5353612L, 3462315L))
```

example output:

```{r join_ans_head, echo=FALSE}
data.table(KEY = c(8723957L, 8931042L, 3429303L, 8707603L), X2 = c(9424892L, -7723560L, -3180523L, 2339699L), Y2 = c(-8875053L, -4909120L, -8444697L, 8536014L))
```

## Sort description {.tabset .tabset-fade .tabset-pills}

### Notes {.active}

- sort currently tests only soring by integer column

### Test scripts

```{r sort_source, echo=FALSE, results='asis'}
task_source("sort", DT)
```

### Data

data size: 

```{r sort_data_size, echo=FALSE,}
metadata[task=="sort", .N, .(rows, gb)][, .(`Rows`=rows, GB=gb)]
```

example input:

```{r sort_data_head, echo=FALSE}
data.table(KEY = c(829673L, 5501052L, 9635168L, 7069052L), X2 = c(-92335L, -8190789L, -6631465L, -1289657L))
```

example output:

```{r sort_ans_head, echo=FALSE}
data.table(KEY = c(0L, 3L, 6L, 7L), X2 = c(1624390L, -5284359L, 9592786L, 159518L))
```


## Group by description {.tabset .tabset-fade .tabset-pills}

### Notes {.active}

- pydatatable did not yet support grouping by multiple columns so one of the questions could not be answered with it [h2oai/datatable#1082](https://github.com/h2oai/datatable/issues/1082)

### Test scripts

```{r groupby_source, echo=FALSE, results='asis'}
task_source("groupby", DT)
```

### Data

data size: 

```{r groupby_data_size, echo=FALSE,}
metadata[task=="groupby", .N, .(rows, gb)][, .(`Rows`=rows, GB=gb)]
```

example input:

```{r groupby_data_head, echo=FALSE}
data.table(id1 = c("id266", "id373", "id573", "id909"), id2 = c("id410", "id791", "id605", "id022"), id3 = c("id0000009560", "id0000005226", "id0000000821", "id0000007739"), id4 = c(180L, 683L, 240L, 511L), id5 = c(69L, 50L, 785L, 818L), id6 = c(5703L, 2003L, 2893L, 5755L), v1 = c(1L, 4L, 5L, 1L), v2 = c(1L, 1L, 2L, 3L), v3 = c(66.5912, 83.3882, 24.3, 24.8819))
```

example output for a `sum v1 by id1:id2` question:

```{r groupby_ans_head, echo=FALSE}
data.table(id1 = c("id266", "id373", "id573", "id909"), id2 = c("id410", "id791", "id605", "id022"), v1 = c(26L, 47L, 31L, 32L))
```

------

## Benchmark setup

#### Hardware

20 cores and 125GB memory  

#### Software version

- data.table - [latest dev](https://github.com/Rdatatable/data.table) - `r dt[solution=="data.table", solution.date(solution[.N], version[.N], git[.N])]`
- dplyr - [latest dev](https://github.com/tidyverse/dplyr) - `r dt[solution=="dplyr", solution.date(solution[.N], version[.N], git[.N])]`
- pandas - [recent stable](https://pypi.python.org/pypi/pandas) - `r dt[solution=="pandas", solution.date(solution[.N], version[.N], git[.N])]`
- pydatatable - [latest dev](https://github.com/h2oai/datatable) - `r dt[solution=="pydatatable", solution.date(solution[.N], version[.N], git[.N])]`

#### Tuning

We placed the following in `~/.R/Makevars` before installing the packages:
```
CFLAGS=-O3 -mtune=native
CXXFLAGS=-O3 -mtune=native
```

Otherwise, R by default compiles packages with `-g -O2`. The R core team know of some problems with `-O3` with some other CRAN packages in some circumstances so that choice of R's default is not an oversight. On the other hand we don't know which CRAN packages are affected and to what degree. So we used `-O3` for these benchmarks to isolate that concern, since we're not aware of any problem with `-O3`.  
Python appears to use `-O3` by default.

#### Notes

- none so far

------

## FAQ

> Why did we do our own tests rather then TPC?

We wanted to measure scale factor both for data volume and data complexity. We can say we built on the peak of the hardest mountain first. If h2o can do that, then h2o can build lower down the mountain. TPC are specialized tests which, depending on the TPC type, meant to cover scalbility for various use cases, not precisely the extreme cases from the technical point of view. We do plan to release TPC benchmark.  

> What does `cache=TRUE|FALSE` cases means?

As multinode tools has been currently removed from benchmark this field should always be `FALSE`.  
Some of the tools doesn't have ability to keep query result in cache, which is useful if you use the tool for ETL purposes. Currently those tools are impala and presto and they will have extra timings entries, one for `cache=FALSE` and another for `cache=TRUE`. The latter one is basically `CREATE TABLE AS SELECT` which adds extra overhead, therefore timings used on reports are using first, faster `cache=FALSE` timing where possible, which is not always possible due to query optimizer. If you are going to use `cache==TRUE` scenario more in your workflow be sure to confirm your conclusions against complete timings data on `cache` granularity provided together with the document. Keep in mind that `cache=FALSE` will not guarantee that query results are fully materialized for impala and presto.  

> Why populating data for join/sort test is not straightfoward?

We use [PCG random number generator](http://www.pcg-random.org/), in the [tableGen.c](https://github.com/h2oai/db-benchmark/blob/master/tableGen.c) file you have the code you can use to produce the data. Rationale behind that decision can be found in [H2O@NYC presentation](https://youtu.be/5X7h1rZGVs0?t=750).  

> How was memory usage measured?

We needed to use measure that will be portable across different software. `ps -o rss $pid` while not really precise is perfectly portable, thus it was used from R and python to capture RSS memory measure after each query run. Before each query there is call to garbage collector.  

------

## About

### Timings

Benchmark timings are being collected over a period of `r paste(range(DT$datetime), collapse=" - ")`. Unless stated otherwise, plots presented in this document are based on most recent benchmark timings. Those were collected over a period of `r paste(range(dt$datetime), collapse=" - ")`.  

### Most recent timings

These are timings from most recently run benchmark workflow. They are additionally filtered to keep only first `cache` scenario, when two present - related to impala and presto. To get `cache` granularity timings use _all available timings data_ below.  

```{r recent_data, echo=FALSE}
DT::datatable(dt, extensions = 'Buttons', options = list(
  pageLength = 6, autoWidth = TRUE, lengthMenu = c(3, 6, 12),
  dom = 'Bfrtip', buttons = c('csv','print')
))
```

### All available timings data

Entries provided in full timings dump may contain runs from shared environment, so is not really reliable. Thus report shows only last benchmark run timings, which has to be made on dedicated environment.  
Filter to most recent timings using `data.table::fread("db-benchmark.csv")[order(timestamp), .SD[.N], by=.(task, data, in_rows, question, solution, fun, run, cache)]`. To avoid _double counting_ caused by `cache=T|F` add chain with `.SD[1L]` / `.SD[.N]` and group without `cache` and `fun` fields.  

```{r all_data, echo=FALSE}
DT::datatable(DT, extensions = 'Buttons', options = list(
  pageLength = 6, autoWidth = TRUE, lengthMenu = c(3, 6, 12),
  dom = 'Bfrtip', buttons = c('csv','print')
))
```

### Fields description

- `batch`: integer timestamp of the workflow run for multiple solutions, tasks, datasets and volumes.  
- `timestamp`: numeric value of system timestamp, seconds from 1970-01-01.
- `task`: benchmark operation.  
- `data`: csv filename, for join two filenames.  
- `in_rows`: nrow of input dataset, if multiple then only first provided.  
- `question`: query question, currently used for *group by* tests.  
- `out_rows`: nrow of results from query.  
- `out_cols`: ncol of results from query.  
- `solution`: a tool used to solve the task.  
- `version`: version of a tool used.  
- `git`: git commit hash for a tool for precise development version tracking.  
- `fun`: function or method used for a tool.  
- `run`: each query is run 3 times.  
- `time_sec`: timing of a query in seconds.  
- `mem_gb`: memory used during processing.  
- `cache`: see FAQ.  
- `chk`: total sum for each measure in the query result, used for validation of results.  
- `chk_time_sec`: time taken to calculate `chk` field, last resort force evaluation, not available when `cache=FALSE`.  
- `comment`: field not used in the workflow, a placeholder for comments related to particular timing entry.  
- `datetime`: value of `timestamp` field as `POSIXct` for human readable print.  

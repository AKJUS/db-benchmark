---
title: "Single-node data aggregation benchmark"
output:
  html_document:
    dev: svg
---

This page aims to benchmark various database-like tools popular in open-source data science: pandas, dplyr and data.table. It runs regularly against very latest versions of these packages and automatically updates. We provide this as a service to both developers of these packages and to users. We hope to add joins and updates with a focus on ordered operations which are hard to achieve in (unordered) SQL. We hope to add Julia too and are looking for help to do so.

We limit the scope to what can be achieved on a single machine. Laptop size ram (8GB) and server size ram (250GB) are in scope. Out-of-memory using local disk such as NVMe is in scope as these packages start to begin to support that (notably pydatatable). Multi-node systems such as Spark running in single machine mode is in scope, too. We believe the multi-node ability of distributed systems incurs a cost that impacts performance when distribution isn't needed; one goal of this page is to measure that cost. Machines are getting bigger: EC2 X1 has 2TB RAM and 1TB NVMe disk is under $300. If you can perform the task on a single machine, then perhaps you should. To our knowledge, nobody has yet compared this software in this way and published results too.

Unlike TPC benchmarks we report timings. Yes: actual timings in seconds/minutes intended for easy human consumption. We also include the syntax being timed alongside the timing. This way you can immediately see whether you are doing these tasks or not, and if the timing differences matter to you or not. A 10x difference may be irrelevant if that's just 1s vs 0.1s on your data size. The intention is that you click the tab for the size of data you have.

Because we have been asked many times to do so, the first task and initial motivation for this page, was to update the benchmark designed and run by Matt Dowle (creator of `data.table`) in 2014 [here](https://github.com/Rdatatable/data.table/wiki/Benchmarks-%3A-Grouping). The methodology and reproducible code can be obtained there. Exact code of this benchmark can be found at [h2oai/db-benchmark](https://github.com/h2oai/db-benchmark).  
In case of doubts, questions, feedback, feel free to fill an issue in benchmark repository.
Presently [Matt Dowle](https://github.com/mattdowle) and me, [Jan Gorecki](https://github.com/jangorecki), author of this benchmark, are working at [H2O.AI](https://www.h2o.ai) which funded development of this benchmark and infrastructure to run it.

```{r load_deps, include=FALSE}
# Rscript -e 'rmarkdown::render("index.Rmd")'
knitr::opts_chunk$set(cache=FALSE)
library(data.table)
source("helpers.R")
source("benchplot.R") # also creates 'code' for groupby
```

```{r exceptions, include=FALSE}
exceptions = rbindlist(list(
  data.table(solution = "pandas", version="0.23.4", task = "groupby", in_rows = 1e9, data=c("G1_1e9_1e2.csv"), comment = "lack of memory to read csv")#,
    #data.table(solution = "pandas", version="0.23.4", task = "join", in_rows = 1e9, data=c("X1e9_2c.csv-Y1e9_2c.csv"), comment = "lack of memory"),
  #data.table(solution = "dplyr", version="0.7.99.9000", task = "join", in_rows = 1e9, data=c("X1e9_2c.csv-Y1e9_2c.csv"), comment = "Cannot allocate memory"),
  #data.table(solution = "pydatatable", version="0.6.0", task = "join", in_rows = c(1e7,1e8,1e9), data=c("X1e7_2c.csv-Y1e7_2c.csv","X1e8_2c.csv-Y1e8_2c.csv","X1e9_2c.csv-Y1e9_2c.csv"), comment = "not yet implemented")
))
```

```{r load_data, include=FALSE}
# all timings
DT = read_timing("time.csv", col=NULL)
# last benchmark batch
dt = DT[!is.na(batch)][batch==max(batch)] # take last batch only
dt = dt[in_rows %in% c(1e7, 1e8, 1e9)] # filter out 1e6, and eventually others
```

## Groupby {.tabset .tabset-fade .tabset-pills}

### 0.5GB

```{r o_groupby1, echo=FALSE}
benchplot(1e7, timings=dt, code=groupby.code)
```
![](grouping.1e7.png)

### 5GB

```{r o_groupby2, echo=FALSE}
benchplot(1e8, timings=dt, code=groupby.code)
```
![](grouping.1e8.png)

### 50GB {.active}

```{r o_groupby3, echo=FALSE}
benchplot(1e9, timings=dt, code=groupby.code)
```
![](grouping.1e9.png)

## Hardware configuration

20 CPU  
125GB memory  

------

Report was generated on: `r format(Sys.time(), usetz=TRUE)`.

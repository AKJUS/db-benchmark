---
title: "db-benchmark"
output:
  html_document:
    dev: svg
---

```{r load_deps, include=FALSE}
stopifnot(sapply(c("DT","jsonlite","scales"), requireNamespace))
library(bit64)
library(data.table)
source("helpers.R")
```

```{r exceptions, include=FALSE}
L64 = bit64::as.integer64
exceptions = rbindlist(list(
  # data.table(solution = "impala", version="2.5.0", task = "join", in_rows = L64(1e10), data="X1e10_2c.csv-Y1e10_2c.csv", comment = "disk space issue"), # it passed but heavily hitting disk space issues, re-run when disk space issue resolved
  data.table(solution = "impala", version="2.5.0", task = "sort", in_rows = L64(5e9), data="X5e9_2c.csv", comment = "~1h"),
  data.table(solution = "impala", version="2.5.0", task = "sort", in_rows = L64(1e10), data="X1e10_2c.csv", comment = "disk space issue"),
  data.table(solution = "dask", version="0.11.0", task = "join", in_rows = L64(c(1e10)), data=c("X1e10_2c.csv-Y1e10_2c.csv"), comment = "~4.5h"),
  data.table(solution = "presto", version="0.150", task = "sort", in_rows = L64(c(5e9, 1e10)), data=c("X5e9_2c.csv","X1e10_2c.csv"), comment = "Out of Memory"),
  data.table(solution = "presto", version="0.150", task = "join", in_rows = L64(1e10), data=c("X1e10_2c.csv-Y1e10_2c.csv"), comment = "loading speed issue"),
  data.table(solution = "dask", version="0.11.0", task = "sort", in_rows = L64(c(1e7,1e8,1e9,5e9,1e10)), data=sprintf("X%se%s_2c.csv", c(rep(1,3),5,1), c(7:9,9:10)), comment = "Not Implemented")
))
```

```{r load_data, include=FALSE}
cols = setNames(scales::hue_pal()(8), c("presto", "data.table", "dplyr", "h2o", "impala", "pandas", "spark", "dask"))
# all timings
DT = rbindlist(lapply(c("time.csv"), read_timing, cols=cols)) # presto timings scrapped manually in separate csv
DT[is.na(chk_time_sec), chk_time_sec:=0
   ][, time_sec := time_sec + chk_time_sec] # checksum included in timings
# recent timings, single cache=FALSE scenario where available
dt = last_timing(x=DT)

metadata = fread("data.csv") # dictionary of data size
```

```{r def_funs, include=FALSE}
plot_op = function(subset_in_rows, subset_task, dt, subset_solution) {
  stopifnot(is.data.table(dt), is.numeric(subset_in_rows), length(subset_in_rows)==1L, is.character(subset_task), length(subset_task)==1L)
  toplot = dt[in_rows==subset_in_rows & task==subset_task]
  if (!missing(subset_solution)) toplot = toplot[solution%in%subset_solution]
  do_exceptions = exists("exceptions") && is.data.table(exceptions)
  if (do_exceptions) {
    msg = exceptions[in_rows==subset_in_rows & task==subset_task]
    if (!missing(subset_solution)) msg = msg[solution%in%subset_solution]
    common = toplot[msg, nomatch=0L, on=c("task","data","in_rows","solution")]
    if (nrow(common)) {
      # this message will be printed to console output while rendering, not to the document, after upgrade version exceptions should be re-confirmed and updated/removed if needed
      common[, message(sprintf("Timings for '%s' has been filtered out due to exception defined: %s", paste(unique(solution), collapse=", "), paste(unique(i.comment), collapse=", ")))]
      toplot = toplot[!msg, on=c("task","data","in_rows","solution")]
    }
    toplot = rbindlist(list(toplot, msg), use.names=TRUE, fill=TRUE)
  }
  if (!nrow(toplot)) return(invisible())
  # report averaging, see description below, it is reported in plot main title, useful to track unexpected averaging, or when `DT` (all timings) passed instead of `dt` (last timings)
  n_obs = sort(unique(toplot[!is.na(timestamp), .N, .(task, in_rows, solution, run)]$N)) # is.na(timestamp) filter outs exceptions
  toplot[,
         .(time_sec=mean(time_sec)), # average over question (related to groupby) and data (not yet used) and cache (only if all timings provided: `DT`, not `dt`)
         .(task, in_rows, solution, version, col, run, exception=comment)
         ][, time_min:=time_sec/60
           ] -> toplot
  stopifnot(toplot[is.na(exception), uniqueN(run), .(solution)]$V1==3L) # incomplete runs makes the plot invalid
  toplot[!is.na(exception) & is.na(run), run := 1L] # fill exceptions with run=1
  # use secs/min dynamically
  if (max(toplot$time_sec, na.rm=TRUE) <= 3*60) {
    toplot[, time := time_sec]
    time_unit = "Seconds"
  } else {
    toplot[, time := time_min]
    time_unit = "Minutes"
  }
  toplot[, `:=`(solution = paste(solution, substr(version, 1, 8)))][, version:=NULL]
  toplot[run==1, ord := rank(time)][, ord := ord[1L], .(task, in_rows, solution, col)] # recycle order for exceptions
  par(mar=c(2.5,8,4,2)+0.1)
  xrn = range(c(0,toplot$time), na.rm=TRUE)
  nsol = uniqueN(toplot$solution)
  toplot[run==1, plot(time,ord,pch=19,cex=3,ylim=c(0,.N),xlim=xrn,col="red",axes=FALSE,ann=FALSE)]
  toplot[run==1, mtext(exception, side=4, at=seq_along(solution), font=1.25, adj=1, las=1, cex=1.25)]
  toplot[run==2, points(time,ord,pch=19,cex=2.5,col="blue")]
  toplot[run==3, points(time,ord,pch=19,cex=2,col="green")]
  axis(1, cex.axis=1.5, font=2, padj=0.5, line=-1)
  toplot[run==1][order(ord), mtext(solution, side=2, at=seq_along(solution), font=1.25, las=1, cex=1.25)]
  mtext(paste0(time_unit, "  "), side=2, 
        at=if (nsol==8) -0.65 else if (nsol==5) -0.4 else -0.5, # dynamic based on number of solutions, tested on 8 and 5
        las=1, cex=1.5, font=2)
  gb = metadata[task==subset_task & rows==subset_in_rows, sum(gb)]
  if (gb > 1) gb = round(gb, 0)
  gb = paste0(gb, "GB")
  title(main = sprintf("%s %s%s", gb, subset_task,
                       if (identical(n_obs, 1L)) "" else paste0(" (mean over ",paste(n_obs, collapse=",")," obs.)")))
  invisible()
}

task_source = function(subset_task, DT) {
  stopifnot(length(subset_task)==1L)
  cat(sapply(DT[task==subset_task, unique(solution)], function(x) {
    # remove dots from solution name
    fp = file.path("https://github.com/h2oai/db-benchmark/blob/master", xx<-gsub(".", "", x, fixed=TRUE), sprintf("%s-%s.%s", subset_task, xx, file.ext(x)))
    sprintf("- [%s](%s)", basename(fp), fp)
  }), sep="  \n")
}
```

## Benchmark overview

Repository for reproducible benchmarking of database-like operations.  
Benchmark is mainly focused on portability and reproducibility, there was no *production tuning* made. I encourage readers to re-run benchmark on own cluster with chosen tools, see [h2oai/db-benchmark](https://github.com/h2oai/db-benchmark) readme for details. Pull requests are welcome, any tunings proposed should be portable.  

Time includes the actual query, materializing results on the query engine side, time of the count rows of result. To ensure that query is evaluated also the sum of measures produced by each query is included in timing.  
Size of data mentioned in plots refer to on-disk stored csv file, or in case of multiple tables involved, the sum of csv files size.  

Following legend describe rank plots in next sections.  

```{r o_legend, echo=FALSE, fig.width=1.85, fig.height=1.2}
oldmar = par(mar=rep(0,4))
plot(0,0,type="n", axes=F, xlab="", ylab="")
legend("center", legend=c("1st","2nd","3rd"), title="Each query runs 3 times", col=c("red","blue","green"), pch=19, pt.cex=c(3,2.5,2), bty="n", y.intersp = 1.25, inset = 0.1)
par(oldmar)
```

------

## High cardinality big to big join {.tabset .tabset-fade .tabset-pills}

### 1e7

```{r o_join1, echo=FALSE}
plot_op(1e7, "join", dt)
```

### 1e8

```{r o_join2, echo=FALSE}
plot_op(1e8, "join", dt)
```

### 1e9 {.active}

```{r o_join3, echo=FALSE}
plot_op(1e9, "join", dt)
```

### 5e9

```{r o_join4, echo=FALSE, message=FALSE}
plot_op(5e9, "join", dt)
```

### 1e10

```{r o_join5, echo=FALSE, message=FALSE}
plot_op(1e10, "join", dt)
```


## Sort {.tabset .tabset-fade .tabset-pills}

### 1e7

```{r o_sort1, echo=FALSE}
plot_op(1e7, "sort", dt)
```

### 1e8

```{r o_sort2, echo=FALSE}
plot_op(1e8, "sort", dt)
```

### 1e9 {.active}

```{r o_sort3, echo=FALSE}
plot_op(1e9, "sort", dt)
```

### 5e9

```{r o_sort4, echo=FALSE, message=FALSE}
plot_op(5e9, "sort", dt)
```

### 1e10

```{r o_sort5, echo=FALSE, message=FALSE}
plot_op(1e10, "sort", dt)
```

## Group by {.tabset .tabset-fade .tabset-pills}

### 1e7

```{r o_groupby1, echo=FALSE}
plot_op(1e7, "groupby", dt)
```

### 1e8

```{r o_groupby2, echo=FALSE}
plot_op(1e8, "groupby", dt)
```

### 1e9 {.active}

```{r o_groupby3, echo=FALSE}
plot_op(1e9, "groupby", dt)
```

### 5e9

```{r o_groupby4, echo=FALSE, message=FALSE}
plot_op(5e9, "groupby", dt)
```

### 1e10

```{r o_groupby5, echo=FALSE, message=FALSE}
plot_op(1e10, "groupby", dt)
```



------

## Benchmark description

## Join description {.tabset .tabset-fade .tabset-pills}

### Notes {.active}

- presto is known to not well address big join (high cardinality), as read [here](http://dataconomy.com/presto-versus-hive-what-you-need-to-know/)
- impala uses `STRAIGHT_JOIN` clause and `SHUFFLE` hint, x4 speed up on 5e9 rows join
- dask scalability for 1e10 reported in [dask#1545](https://github.com/dask/dask/issues/1545)

### Test scripts

```{r join_source, echo=FALSE, results='asis'}
task_source("join", DT)
```

### Data

data size: 

```{r join_data_size, echo=FALSE,}
metadata[task=="join", .N, .(rows, gb)][, .(`Rows`=rows, GB=gb)]
```

example input:

```{r join_data_head, echo=FALSE}
cat("X:")
data.table(KEY = c(829673L, 5501052L, 9635168L, 7069052L), X2 = c(-92335L, -8190789L, -6631465L, -1289657L))
cat("Y:")
data.table(KEY = c(6101982L, 8723957L, 3409724L, 230673L), Y2 = c(3226855L, -8875053L, 5353612L, 3462315L))
```

example output:

```{r join_ans_head, echo=FALSE}
data.table(KEY = c(8723957L, 8931042L, 3429303L, 8707603L), X2 = c(9424892L, -7723560L, -3180523L, 2339699L), Y2 = c(-8875053L, -4909120L, -8444697L, 8536014L))
```

## Sort description {.tabset .tabset-fade .tabset-pills}

### Notes {.active}

- impala timings for 5e9+ are likely affected by lack of free disk space on d2-precise1 machine
- impala uses workaround `ROW_NUMBER() OVER (ORDER BY ...)` as impala-cli is unable to perform *ORDER BY* without printing all results from the query, see commented blocks in [sort-impala.sql](https://github.com/h2oai/db-benchmark/blob/master/impala/sort-impala.sql)
- impala sort tests only `cache=TRUE` scenario because `cache=FALSE` tests were unable to materialize sort results
- presto requires to fit all data into single node according to [presto query faq: avoid order by](https://docs.treasuredata.com/articles/presto-query-faq#solution-avoid-order-by) so it won't scale for big sort, testing on 5e9 result into `failed: 2147483639`, and for 1e9 took ~2h, looking forward on [presto#6042](https://github.com/prestodb/presto/issues/6042)
- dask does not provide sort method, for details see [dask#1414](https://github.com/dask/dask/issues/1414)

### Test scripts

```{r sort_source, echo=FALSE, results='asis'}
task_source("sort", DT)
```

### Data

data size: 

```{r sort_data_size, echo=FALSE,}
metadata[task=="sort", .N, .(rows, gb)][, .(`Rows`=rows, GB=gb)]
```

example input:

```{r sort_data_head, echo=FALSE}
data.table(KEY = c(829673L, 5501052L, 9635168L, 7069052L), X2 = c(-92335L, -8190789L, -6631465L, -1289657L))
```

example output:

```{r sort_ans_head, echo=FALSE}
data.table(KEY = c(0L, 3L, 6L, 7L), X2 = c(1624390L, -5284359L, 9592786L, 159518L))
```


## Group by description {.tabset .tabset-fade .tabset-pills}

### Notes {.active}

- dask does not yet allow to answer all our grouping questions efficiently, workaround suggested in [dask#1517](https://github.com/dask/dask/issues/1517) was applied on `sum v1 mean v3 by id3`, `mean v1:v3 by id4`, `sum v1:v3 by id6` questions

### Test scripts

```{r groupby_source, echo=FALSE, results='asis'}
task_source("groupby", DT)
```

### Data

data size: 

```{r groupby_data_size, echo=FALSE,}
metadata[task=="groupby", .N, .(rows, gb)][, .(`Rows`=rows, GB=gb)]
```

example input:

```{r groupby_data_head, echo=FALSE}
data.table(id1 = c("id266", "id373", "id573", "id909"), id2 = c("id410", "id791", "id605", "id022"), id3 = c("id0000009560", "id0000005226", "id0000000821", "id0000007739"), id4 = c(180L, 683L, 240L, 511L), id5 = c(69L, 50L, 785L, 818L), id6 = c(5703L, 2003L, 2893L, 5755L), v1 = c(1L, 4L, 5L, 1L), v2 = c(1L, 1L, 2L, 3L), v3 = c(66.5912, 83.3882, 24.3, 24.8819))
```

example output for a `sum v1 by id1:id2` question:

```{r groupby_ans_head, echo=FALSE}
data.table(id1 = c("id266", "id373", "id573", "id909"), id2 = c("id410", "id791", "id605", "id022"), v1 = c(26L, 47L, 31L, 32L))
```


------

## Benchmark setup

#### Hardware

10 nodes environment, 32 cores and 200GB memory each node.

#### Softrware

- h2o on 10 nodes
- spark on 10 nodes: 9 workers, 1 master and driver
- impala on 10 nodes: 9 daemons, 1 state store and catalog server
- data.table on 1 node
- pandas on 1 node
- pydatatable on 1 node
- dask on 10 nodes: 9 workers, 1 scheduler
- dplyr on 1 node
- presto on 10 nodes: 9 workers, 1 coordinator

#### Software version

<!-- - h2o - [latest dev](https://github.com/h2oai/db-benchmark/tree/master/h2o/init-h2o.sh) - `_r dt[solution=="h2o", solution.date(solution[.N], version[.N], git[.N])]` -->
- data.table - [latest dev](https://github.com/h2oai/db-benchmark/tree/master/datatable/init-datatable.sh) - `r dt[solution=="data.table", solution.date(solution[.N], version[.N], git[.N])]`
<!-- - spark - [recent-ish dev](http://people.apache.org/~pwendell/spark-nightly/spark-master-bin/latest/) - `_r dt[solution=="spark", solution.date(solution[.N], version[.N], git[.N])]` -->
<!-- - impala - recent-ish stable - `_r dt[solution=="impala", solution.date(solution[.N], version[.N], git[.N])]` -->
- pandas - [recent stable](https://pypi.python.org/pypi/pandas) - `r dt[solution=="pandas", solution.date(solution[.N], version[.N], git[.N])]`
<!-- - dask - [recent stable](https://pypi.python.org/pypi/dask) - `_r dt[solution=="dask", solution.date(solution[.N], version[.N], git[.N])]` -->
- dplyr - [recent stable](https://cran.r-project.org/package=dplyr) - `r dt[solution=="dplyr", solution.date(solution[.N], version[.N], git[.N])]`
- pydatatable - [recent stable](https://github.com/h2oai/datatable) - `r dt[solution=="pydatatable", solution.date(solution[.N], version[.N], git[.N])]`
<!-- - presto - recent-ish stable - `_r dt[solution=="presto", solution.date(solution[.N], version[.N], git[.N])]` -->

#### Tuning

- spark
    - `spark.driver.maxResultSize=200g`
    - `increate spark.network.timeout=2400`
    - `spark.executor.heartbeatInterval=1200`

- presto
    - `hive.metastore-cache-ttl=0s`
    - `distributed-joins-enabled=true`
    - `query.max-memory=1600GB`
    - `query.max-memory-per-node=200GB`
    - `resources.reserved-system-memory=24GB`
    - `-Xmx230G`
    - `-Xms230G`

#### Notes

- impala is running using lxc  
- impala and presto reads from disk (parquet, orc)  
- impala and presto mostly runs in cache TRUE/FALSE scenarios - on reports `cache=FALSE` (faster) is used when possible but there is no guarantee that query results gets fully materialized  
- data.table, dplyr and pandas are single node solutions, they are included in plots up to 1e9 rows  
- presto timings are at the moment ([presto#5857](https://github.com/prestodb/presto/issues/5857)) hand scraped from logs
- spark timings can vary within the single reported version due to lack of unique identifier of spark devel snapshots ([SPARK-16864](https://issues.apache.org/jira/browse/SPARK-16864))  
- pandas, data.table and dplyr were unable to process 5e9+ rows tests due to [pandas#14131](https://github.com/pydata/pandas/issues/14131) and 2^31 limit in `fread`  
- presto uses hive (>= 0.13.0) to create tables on which it issue queries
- presto does not have 1e10 tests due to huge data loading time required by presto, more details in [scalability issue 20GB/h](http://stackoverflow.com/questions/39309984/csv-loading-from-hadoop-to-hive-scalability-issue-20gb-h)
- reported *Out of Memory* on 5e9 and 1e10 rows was really reproduced only on 5e9 rows, please let me know if such assumption is not valid

------

## FAQ

> Why did we do our own tests rather then TPC?

We wanted to measure scale factor both for data volume and data complexity. We can say we built on the peak of the hardest mountain first. If h2o can do that, then h2o can build lower down the mountain. TPC are specialized tests which, depending on the TPC type, meant to cover scalbility for various use cases, not precisely the extreme cases from the technical point of view. We do plan to release TPC benchmark.  

> Why number of workers varies between solutions?

We have 10 machines for running benchmark. Dependending on the architecture of solutions some of machines needs to be coordinator/master/driver/etc. nodes.  

> What does `cache=TRUE|FALSE` cases means?

Some of the tools doesn't have ability to keep query result in cache, which is useful if you use the tool for ETL purposes. Currently those tools are impala and presto and they will have extra timings entries, one for `cache=FALSE` and another for `cache=TRUE`. The latter one is basically `CREATE TABLE AS SELECT` which adds extra overhead, therefore timings used on reports are using first, faster `cache=FALSE` timing where possible, which is not always possible due to query optimizer. If you are going to use `cache==TRUE` scenario more in your workflow be sure to confirm your conclusions against complete timings data on `cache` granularity provided together with the document. Keep in mind that `cache=FALSE` will not guarantee that query results are fully materialized for impala and presto.  

> Why Google BigQuery and Amazon Redshift are no included?

Both BigQuery and Redshift aren't open source, neither can be run on-premises. Please ping me when Google or Amazon will manage to open their platforms, I will be glad to add them.  

> Is it fair to compare distributed solutions to single machine solutions?

It isn't, yet it is still good to have them included. Single machine solutions are usually more commonly known, many people do prototypes in single machine tools thus can have a reference on performance difference when moving to distributed solutions. Readers should keep in mind that single node tools won't scale as good as distributed ones.  

> Why I'm not able to reproduce benchmark for impala?

Among currently tested tools only Impala wasn't setup in a portable way - [is it even possible?](http://stackoverflow.com/q/39087813/2490497). We run impala tests on pre-installed CDH, unlike other tools which just starts the binaries copied to own directories in `$HOME`. Impala cli is still used from a portable directory in `$HOME`.  

> Why populating data for join/sort test is not straightfoward?

We use [PCG random number generator](http://www.pcg-random.org/), in the [tableGen.c](https://github.com/h2oai/db-benchmark/blob/master/tableGen.c) file you have the code you can use to produce the data. Rationale behind that decision can be found in [H2O@NYC presentation](https://youtu.be/5X7h1rZGVs0?t=750).  

------

## About

### Timings

Benchmark timings are being collected over a period of `r paste(range(DT$datetime), collapse=" - ")`. Unless stated otherwise, plots presented in this document are based on most recent benchmark timings. Those were collected over a period of `r paste(range(dt$datetime), collapse=" - ")`.  

### Most recent timings

These are timings from most recently run benchmark workflow. They are additionally filtered to keep only first `cache` scenario, when two present - related to impala and presto. To get `cache` granularity timings use _all available timings data_ below.  

```{r recent_data, echo=FALSE}
DT::datatable(dt, extensions = 'Buttons', options = list(
  pageLength = 6, autoWidth = TRUE, lengthMenu = c(3, 6, 12),
  dom = 'Bfrtip', buttons = c('csv','print')
))
```

### All available timings data

Entries provided in full timings dump may contain runs from shared environment, so is not really reliable. Thus report shows only last benchmark run timings, which has to be made on dedicated environment.  
Filter to most recent timings using `data.table::fread("db-benchmark.csv")[order(timestamp), .SD[.N], by=.(task, data, in_rows, question, solution, fun, run, cache)]`. To avoid _double counting_ caused by `cache=T|F` add chain with `.SD[1L]` / `.SD[.N]` and group without `cache` and `fun` fields.  

```{r all_data, echo=FALSE}
DT::datatable(DT, extensions = 'Buttons', options = list(
  pageLength = 6, autoWidth = TRUE, lengthMenu = c(3, 6, 12),
  dom = 'Bfrtip', buttons = c('csv','print')
))
```

### Fields description

- `batch`: integer timestamp of the workflow run for multiple solutions, tasks, datasets and volumes.  
- `timestamp`: numeric value of system timestamp, seconds from 1970-01-01.
- `task`: benchmark operation.  
- `data`: csv filename, for join two filenames.  
- `in_rows`: nrow of input dataset, if multiple then only first provided.  
- `question`: query question, currently used for *group by* tests.  
- `out_rows`: nrow of results from query.  
- `solution`: a tool used to solve the task.  
- `version`: version of a tool used.  
- `git`: git commit hash for a tool for precise development version tracking.  
- `fun`: function or method used for a tool.  
- `run`: each query is run 3 times.  
- `time_sec`: timing of a query in seconds.  
- `mem_gb`: memory used during processing.  
- `cache`: see FAQ.  
- `chk`: total sum for each measure in the query result, used for validation of results.  
- `chk_time_sec`: time taken to calculate `chk` field, last resort force evaluation, not available when `cache=FALSE`.  
- `comment`: field not used in the workflow, a placeholder for comments related to particular timing entry.  
- `col`: color used to represent solution, unique within solution.  
- `datetime`: value of `timestamp` field as `POSIXct` for human readable print.  


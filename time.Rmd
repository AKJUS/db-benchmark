
Time includes the actual query, materializing results on the query engine side, and time of the count rows of result.

## Setup

- h2o on 9 nodes
- spark on 10 nodes: 8 executors, 1 manager, 1 client-driver
- impala on 9 nodes: 8 daemons, 1 state store and catalog server
- data.table on 1 node
- pandas on 1 node

32 cores and 200GB memory each node.

## Software version

- h2o 3.9.1.99999 - latest
- data.table 1.9.7 - latest
- spark 2.0.0 preview
- impala 2.5.0 - lastest
- pandas 0.16.2 - latest on Ubuntu 12.04 (Jun 2015)

## Tuning

- impala
  - `STRAIGHT_JOIN` and `SHUFFLE` hint, see [Query Hints in Impala SELECT Statements](http://www.cloudera.com/documentation/enterprise/latest/topics/impala_hints.html)
  
- spark
  - `spark.driver.maxResultSize=200g`
  - `increate spark.network.timeout=1800`
  - `spark.executor.heartbeatInterval=600`

## Benchmark

```{r init, echo=FALSE, message=FALSE}
library(data.table)
dt = fread(Sys.getenv("CSV_TIME_FILE", "time.csv"), sep=",") # file stores multiple batches of benchmark run, so the filtering below
dt[order(solution), col := .GRP+1L, .(solution)] # assign colors to solutions

# impala compute stats
impala_stats = dt[solution%in%c("h2o","impala") & in_rows==1e9L]
dt = dt[fun!="COMPUTE STATS INNER JOIN"] # filter out
# boxplot(time_sec ~ solution,
#         data=impala_stats[fun=="COMPUTE STATS INNER JOIN", solution := "impala stats"],
#         main=sprintf("impala compute stats vs h2o on %.e rows", impala_stats$in_rows[1L]),
#         ylab="seconds")

grain = c("task","data","in_rows","out_rows","solution","col","fun")

# only most recent benchmark batch
recent_batch = dt[, tail(.SD, 1L), by=c(grain,"run")]

# only min timing from 3 runs in recent batch
imin = recent_batch[, .I[which.min(time_sec)], keyby=c(grain)]$V1 # take which.min
min_of_3_recent_batch = recent_batch[imin]

# only first run for each tool
first_run_recent_batch = recent_batch[run==1L]

# only max NROW batch
max_nrow_recent_batch = recent_batch[in_rows==max(in_rows)] # subset for only bigger NROW
```

### big join NxN

First run time grouped with min time of 3 runs across data volumes.

```{r join_all, echo=FALSE, fig.width=10, fig.height=10}
first_and_min_recent_batch = rbindlist(list(
  first_run_recent_batch[, .(run_opt="first run", time_sec), grain],
  min_of_3_recent_batch[, .(run_opt="min of 3 runs", time_sec), grain]
))

op=par(mfrow=c(uniqueN(first_and_min_recent_batch$in_rows), 1L))
lapply(unique(first_and_min_recent_batch$in_rows), function(v_in_rows) {
  dd = dcast(first_and_min_recent_batch[in_rows==v_in_rows], run_opt ~ solution, value.var="time_sec")
  mm = as.matrix(dd[, !"run_opt", with=FALSE])
  row.names(mm) <- dd$run_opt
  barplot(mm,
          beside=TRUE,
          # lookup colors and recycle on for each pair of bars
          col=first_and_min_recent_batch[colnames(mm), rep(col,2L), on="solution", by=.EACHI, mult="first"]$V1,
          main=sprintf("big join %.e rows (first, min of 3)", v_in_rows))
}) -> nul
```

First run timing across volume.

```{r join_x_volume, echo=FALSE, fig.width=10, fig.height=8}
par(mfrow=c(1,1))
recent_batch[run==1L,
             plot(in_rows, time_sec, 
                  type="n", xlab="rows", ylab="seconds",
                  main="min join timing over volume",
                  xaxt="n",
                  log="xy")] -> nul
axis(side = 1, at = recent_batch[run==1L, sort(unique(in_rows))])
lapply(split(recent_batch[run==1L], by="solution"),
       function(x) x[,lines(in_rows, time_sec, col=col)]) -> nul
recent_batch[run==1L, .(col=col[1L]), solution][, legend("topleft", legend=solution, col=col, lty=1)] -> nul
```

1e9 rows timing across all benchmark runs including historical.

```{r all_max_nrow_timing, echo=FALSE, fig.width=10, fig.height=8}
par(mfrow=c(1,1))
max_in_rows = dt[, max(in_rows)]
knitr::kable(dt[in_rows==max_in_rows, .(times_benchmarked=.N), solution], row.names=FALSE, caption="benchmark timing entries")
boxplot(time_sec ~ solution,
        data=dt[in_rows==max_in_rows],
        main=sprintf("timing of all previous benchmark runs on %.e rows", max_in_rows),
        ylab="seconds")
```

Timing data.

```{r timing_data, echo=FALSE}
print(dt[, datetime := as.POSIXct(timestamp, origin="1970-01-01")][])
```
